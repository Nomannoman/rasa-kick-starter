
Abstract

Fault-tolerant quantum computers offer the promise of dramatically improving machine
learning through speed-ups in computation or improved model scalability. In the near-term,
however, the benefits of quantum machine learning are not so clear. Understanding expressibil-
ity and trainability of quantum models—and quantum neural networks in particular—requires
further investigation. In this work, we use tools from information geometry to define a notion
of expressibility for quantum and classical models. The effective dimension, which depends
on the Fisher information, is used to prove a novel generalisation bound and establish a ro-
bust measure of expressibility. We show that quantum neural networks are able to achieve
a significantly better effective dimension than comparable classical neural networks. To then
assess the trainability of quantum models, we connect the Fisher information spectrum to
barren plateaus, the problem of vanishing gradients. Importantly, certain quantum neural
networks can show resilience to this phenomenon and train faster than classical models due to
their favourable optimisation landscapes, captured by a more evenly spread Fisher information
spectrum. Our work is the first to demonstrate that well-designed quantum neural networks
offer an advantage over classical neural networks through a higher effective dimension and
faster training ability, which we verify on real quantum hardware.

1 Introduction

The power of a model lies in its ability to fit a variety of functions [1]. In machine learning, power
is often referred to as a model’s capacity to express different relationships between variables [2].
Deep neural networks have proven to be extremely powerful models, capable of capturing intricate
relationships by learning from data [3]. Quantum neural networks serve as a newer class of machine
learning models that are deployed on quantum computers and use quantum effects such as super-
position, entanglement, and interference, to do computation. Some proposals for quantum neural
networks include [4-11] and hint at potential advantages, such as speed-ups in training and faster
processing. Whilst there has been much development in the growing field of quantum machine
learning, a systematic study of the trade-offs between quantum and classical models has yet to be
conducted [12]. In particular, the question of whether quantum neural networks are more powerful
than classical neural networks, is still open.

A common way to quantify the power of a model is by its complexity [13]. In statistical learn-
ing theory, the Vapnik-Chervonenkis (VC) dimension is an established complexity measure, where
error bounds on how well a model generalises (i.e., performs on unseen data), can be derived [14].
Although the VC dimension has attractive properties in theory, computing it in practice is noto-
riously difficult. Further, using the VC dimension to bound generalisation error requires several
unrealistic assumptions, including that the model has access to infinite data [15,16]. The measure
also scales with the number of parameters in the model and ignores the distribution of data. Since
modern deep neural networks are heavily overparameterised, generalisation bounds based on the
VC dimension, and other measures alike, are typically vacuous.

The authors analysed the expressive power of parameterised quantum circuits using
memory capacity, and found that quantum neural networks had limited advantages over classical
neural networks. Memory capacity is, however, closely related to the VC dimension and is thus,
subject to similar criticisms.

We therefore, turn our attention to measures that are calculable in practice and incorporate the
distribution of data. In particular, measures such as the effective dimension have been motivated
from an information-theoretic standpoint and depend on the Fisher information; a quantity that
describes the geometry of a model’s parameter space and is essential in both statistics and machine
learning [20-22]. We argue that the effective dimension is a robust capacity measure through proof
of a novel generalisation bound with supporting numerical analyses, and use this measure as a tool
to study the power of quantum and classical neural networks.

Despite a lack of quantitative statements on the power of quantum neural networks, another
issue is rooted in the trainability of these models. Often, quantum neural networks suffer from
a barren plateau phenomenon, wherein the loss landscape is perilously flat, and consequently,
parameter optimisation is extremely difficult [23]. As shown in [24], barren plateaus may be noise-
induced, where certain noise models are assumed on the hardware. On the other hand, noise-free
barren plateaus are circuit-induced, which relates to random parameter initialisation, and methods
to avoid them have been explored in [25-28].

A particular attempt to understand the loss landscape of quantum models uses the Hessian
in [29]. The Hessian quantifies the curvature of a model’s loss function at a point in its parameter
space [30]. Properties of the Hessian matrix, such as its spectrum, provide useful diagnostic
information about the trainability of a model [31]. It was also discovered that the entries of
the Hessian, vanish exponentially in models suffering from a barren plateau [32]. For certain
loss functions, the Fisher information matrix coincides with the Hessian of the loss function [33).
Consequently, we examine the trainability of quantum and classical neural networks by analysing
the Fisher information matrix, which is incorporated by the effective dimension. In this way, we
can explicitly relate the effective dimension to model trainability [34].

We find that well-designed quantum neural networks are able to achieve a higher capacity and
faster training ability than comparable classical feedforward neural networks.! Capacity is captured
by the effective dimension, whilst trainability is assessed by leveraging the information-theoretic
properties of the Fisher information. Lastly, we connect the Fisher information spectrum to the
barren plateau phenomenon and find that a quantum neural network with an easier data encoding
strategy, increases the likelihood of encountering a barren plateau, whilst a harder data encoding
strategy shows resilience to the phenomenon.” The remainder of this work is organised as follows.
In Section 2, we discuss the types of models used in this study. Section 3 introduces the effective
dimension from [20] and motivates its relevance as a capacity measure by proving a generalisation
bound. We additionally relate the Fisher information spectrum to model trainability in Section 3.
This link, as well as the power of quantum and classical models, is analysed through numerical
experiments in Section 4, where the training results are further supported by an implementation
on the ibmq-montreal 27-qubit device.

2 Quantum neural networks

Quantum neural networks are a subclass of variational quantum algorithms, comprising of quan-
tum circuits that contain parameterised gate operations [35]. Information is first encoded into a
quantum state via a state preparation routine or feature map [36]. The choice of feature map is
usually geared toward enhancing the performance of the quantum model and is typically neither
optimised nor trained, though this idea was discussed in [37]. Once data is encoded into a quantum
state, a variational model containing parameterised gates is applied and optimised for a particular
task [5-7,38]. This happens through loss function minimisation, where the output of a quantum
model can be extracted from a classical post-processing function that is applied to a measurement
outcome.

 

Paster training implies a model will reach a lower training error than another comparable model for a fixed num-
ber of training iterations. We deem two models comparable if they share the same number of trainable parameters
and the same input and output size.

Figure 1: Overview of the quantum neural network used in this study. The input « € R*=
is encoded into an S-qubit Hilbert space by applying the feature map |7z) := Uz jaye", This
state is then evolved via a variational form |ge{x)) := Go |Wz), where the parameters 6 € © are
chosen to minimise a certain loss function. Finally a measurement is performed whose outcome
z =(z1,...,2g) is post-processed to extract the output of the model y := f(z).

The model we use is depicted in Figure 1. It encodes classical data « € R*™ into an S-qubit
Hilbert space using the feature map U/, proposed in [39]. First, Hadamard gates are applied to each
qubit. Then, normalised feature values of the data are encoded using RZ-gates with rotation angles
equal to the feature values of the data. This is then accompanied by RZZ-gates that encode higher
orders of the data, i.e. the controlled rotation values depend on the product of feature values. The
RZ and RZZ-gates are then repeated.? Once data is encoded, the model optimises a variational
circuit Gg containing parameterised RY-gates with CNOT entangling layers between every pair
of qubits, where 6 € © denotes the trainable parameters. The post-processing step measures all
qubits in the o, basis and classically computes the parity of the output bit strings. For simplicity,
we consider binary classification, where the probability of observing class 0 corresponds to the
probability of seeing even parity and similarly, for class 1 with odd parity. The reason for the
choice of this model architecture is two-fold: the feature map is motivated in [39] to serve as a
useful data embedding strategy that is believed to be difficult to simulate classically as the depth
and width increase*, which we find adds substantial power to a model (as seen in Section 4.2);
and the variational form aims to create more expressive circuits for quantum algorithms [40].
Detailed information about the circuit implementing the quantum neural network is contained in
Appendix A.

We benchmark this quantum neural network against classical feedforward neural networks with
full connectivity and consider all topologies for a fixed number of trainable parameters.” We also
adjust the feature map of the quantum neural network to investigate how data encoding impacts
capacity and trainability. We use a simple feature map that is easy to reproduce classically and
thus, refer to it as an easy quantum model.

3 Information geometry, effective dimension, and trainability of quantum neural networks

We approach the notion of complexity from an information geometry perspective. In doing so,

we are able to rigorously define measures that apply to both classical and quantum models, and
subsequently use them to study the capacity and trainability of neural networks.

 

In general, these encoding operations can be repeated by an arbitrary amount. The amount of repetitions is
termed the depth of the feature map.

This is conjectured to be difficult for depth > 2.

Networks with and without biases and different activation functions are explored. In particular, RELU, leaky
RELU, tanh and sigmoid activations are considered. We keep the number of hidden layers and neurons per layer
variable and initialise with random weights sampled from [—1, 1]?.

®We use a straightforward angle encoding scheme, where data points are encoded via RY-gates on each qubit
without entangling them, with rotations equal to feature values normalised to [—1,1]. See Appendix A for further
details.

The Fisher information presents itself as a foundational quantity in a variety of fields, from physics
to computational neuroscience [41]. It plays a fundamental role in complexity from both a com-
putational and statistical perspective [21]. In computational learning theory, it is used to measure
complexity according to the principle of minimum description length [42]. We focus on a statistical
interpretation, which is synonymous with model capacity: a quantification of the class of functions
a model can fit [1].

A way to assess the information gained by a particular parameterisation of a statistical model
is epitomised by the Fisher information. By defining a neural network as a statistical model, we
can describe the joint relationship between data pairs (2, y) as p(z,y;6) = p(y|x;@)p(2) for all
2eX¥ CR, ye YC R™* and @€ OC [-1,1]*%.” The input distribution, p(x) is a prior
distribution and the conditional distribution, p{y|x;@) describes the input-output relation of the
model for a fixed @ € @. The full parameter space 0 forms a Riemannian space which gives rise
to a Riemannian metric, namely, the Fisher information matrix

By definition, the Fisher
information matrix is positive semidefinite and hence, its eigenvalues are non-negative, real num-
bers.

The Fisher information conveniently helps capture the sensitivity of a neural network’s output
relative to movements in the parameter space, proving useful in natural gradient optimisation—a
method that uses the Fisher information as a guide to optimally navigate through the parameter
space such that a model’s loss declines [43]. In [44], the authors leverage geometric invariances
associated with the Fisher information, to produce the Fisher-Rao norm-a robust norm-based
capacity measure, defined as the quadratic form \| lle. := 6'F (6) for a vectorised parameter
set, 8. Notably, the Fisher-Rao norm acts as an umbrella for several other existing norm-based
measures [45-47] and has demonstrated desirable properties both theoretically, and empirically.

3.2. The effective dimension

The effective dimension is an alternative complexity measure motivated by information geometry,
with useful qualities. The goal of the effective dimension is to estimate the size that a model
occupies in model space-the space of all possible functions for a particular model class, where the
Fisher information matrix serves as the metric. Whilst there are many ways to define the effective
dimension, a useful definition which we apply to both classical and quantum models is presented
in [20]. The number of data observations determines a natural scale or resolution used to observe
model space. This is beneficial for practical reasons where data is often limited, and can help in
understanding how data availability influences the accurate capture of model complexity.

Definition 3.1. The effective dimension of a statistical model Mo := {p(-,-;9) : 9 € O} with
respect to y € (0,1), a d-dimensional parameter space @ C R? and n €N, n > 1 data samples is
defined as


‘This is achieved by applying an appropriate post-processing function in both classical and quantum networks. In
the classical network, we apply a softmax function to the last layer. In the quantum network, we obtain probabilities
based on the post-processing parity function. Both techniques are standard in practice.

8It is important that (aj .45 yey are drawn from the true distribution p(x, y; 8) in order for the empirical Fisher
information to approximate the Fisher information, i.e., img. Fx (8) = F(9) [33]. This is ensured in our numerical
analysis by design.

where the normalisation ensures that ve to tr(F(0))d@ = d.

The effective dimension neatly incorporates the Fisher information spectrum by integrating
over its determinant. There are two minor differences between (2) and the effective dimension
from [20]: the presence of the constant y € (0,1], and the logn term. These modifications are
helpful in proving a generalisation bound, such that the effective dimension can be interpreted as
a bounded capacity measure that serves as a useful tool to analyse the power of statistical models.
We demonstrate this in the following section.

3.3 Generalisation error bounds

Suppose we are given a hypothesis class, H, of functions mapping from ¥V to Y and a training
set Sy = {(21,y1),---, (fn, Yn)} © (¥ x Y)”, where the pairs (2;,y;) are drawn iid. from some
unknown joint distribution p. Furthermore, let D : Y x Y > R be a loss function. The chal-
lenge is to find a particular hypothesis h € H with the smallest possible expected risk, defined
as R(h) := Evey)~pl|L(h(a),y)]. Since we only have access to a training set S,, a good strat-
egy to find the best hypothesis h € H is to minimise the so called empirical risk, defined as
Rn(h) = + Ty L(h(z:), yi). The difference between the expected and the empirical risk is the
generalisation error—an important quantity in machine learning that dictates whether a hypothesis
h €# learned on a training set will perform well on unseen data, drawn from the unknown joint

distribution p [17|. Therefore, an upper bound on the quantity
sup |R(h) — Ra(h)|, (3)
hEH.

which vanishes as n grows large, is of considerable interest. Capacity measures help quantify the
expressiveness and power of #. Thus, the generalisation error in (3) is typically bounded by an
expression that depends on a capacity measure, such as the VC dimension [3] or the Fisher-Rao
norm [44]. Theorem 3.2 provides a novel bound based on the effective dimension, which we use to
study the power of neural networks from hereon.

Bounding generalisation error with the effective dimension In this manuscript, we con-
sider neural networks as models described by stochastic maps, parameterised by some 6 € @.°
The corresponding loss functions are mappings L : P{) x P(Y) — R, where P()) denotes
the set of distributions on Y. We assume the following regularity assumption on the model

Me == {pl-, 30): 6 € O}:
056+ p(-,-;6) is M\-Lipschitz continuous w.r.t. the supremum norm . (4)

Theorem 3.2 (Generalisation bound for the effective dimension). Let 9 = [—1,1]¢ and consider
a statistical model Me := {p(-,-30) : 6 € O} satisfying (4) such that the normalised Fisher
information matrix F(0) has full rank for all 6 € ©, and ||Volog F(9)\| <A for some A > 0 and
all @ € ©. Let dy denote the effective dimension of Me as defined in (2). Furthermore, let
E:P() x P(Y) — [-B/2,B/2] for B > 0 be a loss function that is a-Hélder continuous with
constant M in the first argument w.r.t. the total variation distance for some a € (0,1]. Then
there exists a constant ca, such that for y € (0,1] and alln EN, we have

 

°As a result, the variables h and H are replaced by @ and 9, respectively.
The proof is given in Appendix B.1. Note that the choice of the norm to bound the gradient
of the Fisher information matrix is irrelevant due to the presence of the dimensional constant
Can? If we choose y € (0,1] to be sufficiently small, we can ensure that the right-hand side
of (5) vanishes in the limit n > oo.1! To verify the effective dimension’s ability to capture
generalisation behaviour, we conduct a numerical analysis similar to work presented in [48]. We find
that the effective dimension for a model trained on confusion sets with increasing label corruption,
accurately captures generalisation behaviour. The details can be found in Appendix B.2.

Remark 3.3 (Properties of the effective dimension). In the limit m — oo, the effective dimension
converges to the maximal rank 7 := maxgeo@rg, where rg < d denotes the rank of the Fisher
information matrix F'(@). The proof of this result can be seen in Appendix B.3, but it is worthwhile
to note that the effective dimension does not necessarily increase monotonically with n, as explained
in Appendix B.4.!?

The continuity assumptions of Theorem 3.2 are satisfied for a large class of classical and quan-
tum statistical models [49,50], as well as many popular loss functions. The full rank assumption
on the Fisher information matrix, however, often does not hold in classical models. Non-linear
feedforward neural networks, which we consider in this study, have particularly degenerate Fisher
information matrices [34]. Thus, we further extend the generalisation bound to account for a broad
range of models that may not have a full rank Fisher information matrix.

Remark 3.4 (Relaxing the rank constraint in Theorem 3.2). The generalisation bound in (5) can
be modified to hold for a statistical model without a full rank Fisher information matrix. By parti-
tioning the parameter space O, we discretise the statistical model and prove a generalisation bound
for the discretised version of Me := {p(-,-;6) : 6 € O} denoted by MSs) = {[p')(,-0) 26 € OF,
where « € Nis adiscretisation parameter. By choosing « carefully, we can control the discretisation
error. This is explained in detail, along with the proof, in Appendix B.5.

3.4 The Fisher spectrum and the barren plateau phenomenon

The Fisher information spectrum for fully connected feedforward neural networks reveals that the
parameter space is flat in most dimensions, and strongly distorted in a few others [34]. These
distortions are captured by a few very large eigenvalues, whilst the flatness corresponds to eigen-
values being close to zero. This behaviour has also been reported for the Hessian matrix, which
coincides with the Fisher information matrix under certain conditions [33,51,52].1° These types of
spectra are known to slow down a model’s training and may render optimisation suboptimal [31].
In the quantum realm, the negative effect of barren plateaus on training quantum neural networks
has been linked to the Hessian matrix [32]. It was found that the entries of the Hessian vanish
exponentially with the size of the system in models that are in a barren plateau. This implies that
the loss landscape becomes increasingly flat as the size of the model increases, making optimisation
more difficult.

The Fisher information can also be connected to barren plateaus. Assuming a log-likelihood
loss function, without loss of generality, we can formulate the empirical risk over the full training

 

10In the special case where the Fisher information matrix does not depend on 6, we have A = 0 and (5) holds for
C40 = 2J/d. This may occur in scenarios where a neural network is already trained, i.e., the parameters 6 € © are
fixed.

1lMore precisely, this occurs if y scales at most as y ~ 327aM?/(dB?). To see this, we use the fact that
dyn <d+7/|logn| for some constant Tt > 0.

!2The geometric operational interpretation of the effective dimension only holds if n is sufficiently large. We
conduct experiments over a wide range of n and ensure that conclusions are drawn from results where the choice of
n is sufficient.


where p(y;|2:;8) is the conditional distribution for a data pair (x;,y;).1° From Bayes rule, note
that the derivative of the empirical risk function is then equal to the derivative of the log of the
joint distribution summed over all data pairs, i-e.,


since the prior distribution p(-) does not depend on 6. From [23], we know that we are in a barren
plateau if, for parameters @ uniformly sampled from ©, each element of the gradient of the loss
function with respect to @ vanishes exponentially in the number of qubits, S. In mathematical
terms this means


which implies tr(Ee[F(@)|) < d(ws +w%). Due to the positive semidefinite nature of the Fisher
information matrix and by definition of the Hilbert-Schimdt norm, all matrix entries will approach
zero if a model is in a barren plateau, and natural gradient optimisation techniques become unfea-
sible. We can conclude that a model suffering from a barren plateau will have a Fisher information
spectrum with an increasing concentration of eigenvalues approaching zero as the number of qubits
in the model increase. Conversely, a model with a Fisher information spectrum that is not con-
centrated around zero is unlikely to experience a barren plateau.

We investigate the spectra of quantum and classical neural networks in the following section
and verify the trainability of these models with numerical experiments, including results from real
quantum hardware.

4 Numerical experiments and results

In this section, we compare the Fisher information spectrum, effective dimension and training
performance of the quantum neural network to feedforward models with different topologies. We
also include the easy quantum model with a classically simulable feature map to understand the
impact of data encoding on model expressibility and trainability. Trainability is further verified


14Minimising the empirical risk with a log-likelihood loss function coincides with the task of minimising the relative
entropy D(-||-} between the distribution induced by applying the neural network to the observed input distribution
r and the observed output distribution g. Hence, equivalent to the log-likelihood loss function, we can choose
L(p(ylz; r(x), av) = D(e(y|2; 9)r(z)||a(y)), which fits the framework presented in Section 3.3. We further note
that the relative entropy is a-Hélder continuous in the first argument for a € (0,1). In fact, the relative entropy
is even log-Lipschitz continuous in the first argument, which can be utilised to strengthen the generalisation bound
from Theorem 3.2 as explained in Remark B.3.

15 As is the case with the parity function chosen in the quantum neural network, and the softmax function chosen
in the last layer of the classical neural network.
for the quantum neural network on the ibmq-montreal 27-qubit device available through the
IBM Quantum Experience via Qiskit [53]. In order to do a systematic study, we deem two models

comparable if they share the same number of trainable parameters (d), input size (s;,), and output
size (Sour), and consider d < 100, sin € {4,6, 8,10} with sou; = 2.

4.1 The Fisher information spectrum

Strong connections to capacity and trainability can be derived from the spectrum of the Fisher
information matrix. For each model with a specified triple (d, sin, Sout), we sample 100 sets of
parameters uniformly on © = [—1, 1? and compute the Fisher information matrix 100 times using
a standard Gaussian prior.'© The resulting average distributions of the eigenvalues of these 100
matrices are plotted in Figure 2 for d = 40, sin) = 4 and say = 2.


Figure 2: Average Fisher information spectrum plotted as a histogram for the classical
feedforward neural network and the quantum neural network with two different feature maps.
The plot labelled easy quantum model has a classically simulable data encoding strategy, whilst
the quantum neural network’s encoding scheme is conjectured to be difficult. In each model, we
compute the Fisher information matrix 100 times using parameters sampled uniformly at random.
We fix the number of trainable parameters d = 40, input size sj, = 4 and output size sou, = 2. The
distribution of eigenvalues is the most uniform in the quantum neural network, whereas the other
models contain mostly small eigenvalues and larger condition numbers. This is made more evident
by plotting the distribution of eigenvalues from the first bin in subplots within each histogram
plot.

The classical model’s Fisher information spectrum is concentrated around zero, where the ma-
jority of eigenvalues are negligible!’ , however, there are a few very large eigenvalues. This behaviour
is observed across all classical network configurations that we consider.'* This is consistent with
results from literature, where the Fisher information matrix of non-linear classical neural networks
is known to be highly degenerate, with a few large eigenvalues [34]. The concentration around
zero becomes more evident in the subplot contained in each histogram depicting the eigenvalue
distribution of the first bin. The easy quantum model also has most of its eigenvalues close to zero,
and whilst there are some large eigenvalues, their magnitudes are not as extreme as the classical
model. The quantum neural network, on the other hand, has a different Fisher information spec-
trum. The distribution of eigenvalues is more uniform, with no outlying values and remains more
or less constant as the number of qubits increase (see Appendix C.2). This can be seen from the
range of the eigenvalues on the x-axis in Figure 2 and has implications for capacity and trainability
which we examine next.

 

16A sensitivity analysis is included in Appendix C.1 to verify that 100 parameter samples are reasonable for the
models we consider. In higher dimensions, this number will need to increase.

Specifically, of the order 10~ "4, i.e., close to machine precision and thus, indistinguishable from zero.

18The classical model depicted in Figure 2 is the one with the highest average rank of Fisher information matrices
from all possible classical configurations for a fixed number of trainable parameters, which subsequently gives rise
to the highest effective dimension.
4.2. Capacity analysis

The quantum neural network consistently achieves the highest effective dimension over all ranges of
finite data we consider.'° The reason is due to the speed of convergence, which is slowed down by
smaller eigenvalues and an unevenly distributed Fisher information spectrum. Since the classical
models contain highly degenerate Fisher information matrices, the effective dimension converges
the slowest, followed by the easy quantum model. The quantum neural network, on the other
hand, has a non-degenerate Fisher information matrix and the effective dimension converges to
the maximum effective dimension, d.?° It also converges much faster due to its more evenly spread
Fisher information spectrum. In Figure 3a, we plot the normalised effective dimension for all three

models. The normalisation ensures that the effective dimension lies between 0 and 1 by simply
dividing by d.

Figure 3: (a) Normalised effective dimension plotted for the quantum neural network in
green, the easy quantum model in blue and the classical feedforward neural network in red. We
fix the input size si, = 4, the output size $44; = 2 and number of trainable parameters d = 40.
Notably, the quantum neural network achieves the highest effective dimension over a wide range
of data availability, followed by the easy quantum model. The classical model never achieves an
effective dimension greater or equal to the quantum models for the range of finite data considered
in this study. (b) Training loss. Using the first two classes of the Iris dataset [54], we train
all three models using d = 8 trainable parameters with full batch size. The ADAM optimiser with
an initial learning rate of 0.1 is selected. For a fixed number of training iterations = 100, we train
all models over 100 trials and plot the average training loss along with +1 standard deviation.
The quantum neural network maintains the lowest loss value on average across all three models,
with the lowest spread over all training iterations. Whilst on average, the classical model trains
to a lower loss than the easy quantum model, the spread is significantly larger. We further verify
the performance of the quantum neural network on real quantum hardware and train the model
using the ibmq-montreal 27-qubit device, where the training advantage persists. We plot the
hardware results till they stabilise, at roughly 33 training iterations and find the performance to
be even better than the simulated results.

The quantum neural network outperforms both models, followed by the easy quantum model
and lastly, the classical model. Capacity calculations using the Fisher-Rao norm confirm these
trends. The average Fisher-Rao norm over 100 trials is roughly 250% higher in the quantum
neural network than in the classical neural network, after training the models on a simple dataset
for a fixed number of iterations (see Appendix C.3 for details).

Upon examining the quantum neural network over an increasing system size (see Appendix C.2),
the eigenvalue distribution of the Fisher information matrix remains more or less constant, and a
large amount of the eigenvalues are not near zero, thus, the model shows resilience against barren
plateaus. This is not the case in the easy quantum model. The Fisher information spectrum be-
comes more “barren plateau-like”, with the eigenvalues becoming smaller as the number of qubits
increase. This highlights the importance of the feature map which can influence the likelihood of
experiencing a barren plateau. The higher order feature map used in the quantum neural network
seems to structurally change the optimisation landscape and remove the flatness, usually associ-
ated with barren plateaus or suboptimal optimisation conditions. Classically, the observed Fisher
information spectrum is known to have undesirable optimisation properties where the outlying
eigenvalues slow down training and loss convergence [31].

We confirm the training statements for all three models with an experiment illustrated in
Figure 3b. Using a cross-entropy loss function, optimised with ADAM for a fixed number of training
iterations = 100 and an initial learning rate = 0.1, the quantum neural network trains to a lower
loss, faster than the other two models over an average of 100 trials. To support the promising
training performance of the quantum neural network, we also train it once on real hardware using
the ibmq-montreal 27-qubit device. We reduce the number of CNOT-gates by only considering
linear entanglement instead of all-to-all entanglement in the feature map and variational circuit.
This is to cope with hardware limitations. The full details of the experiment are contained in
Appendix C.4. We find that the quantum neural network is capable of performing even better on
real hardware, thus, tangibly demonstrating faster training.

5 Conclusion

In stark contrast to classical models, understanding the capacity of quantum neural networks is not
well explored. Moreover, classical neural networks are known to produce highly degenerate Fisher
information matrices, which can significantly slow down training. For quantum neural networks,
no such analysis has been done.

In this study, the effective dimension is presented as a robust capacity measure for quantum
and classical models, which we justify through proof of a novel generalisation bound. A particular
quantum neural network offers advantages from both a capacity and trainability perspective. These
advantages are captured by a high effective dimension and a non-degenerate Fisher information
matrix. The feature map in the quantum neural network is conjectured to be hard to simulate
classically, and replacing it with one that is easily simulable, impairs these advantages. This
illustrates the importance of the choice of feature map in designing a powerful quantum neural
network that is able to train well.

Regarding quantum model trainability, the Fisher information spectrum informs us of the
likelihood of experiencing a barren plateau. Changing the feature map, influences the Fisher
spectrum and hence, alters the likelihood of encountering a barren plateau. Again, this points to
the significance of the feature map in a quantum neural network. A model with eigenvalues of the
Fisher information matrix that do not vanish as the number of qubits grow, is unlikely to suffer
from a barren plateau. The quantum neural network with a hard feature map is an example of
such a model showing resilience to this phenomenon with good trainability, supported by results
from real quantum hardware.

This work opens many doors for further research. The feature map in a quantum model plays
a large role in determining both its capacity and trainability via the effective dimension and Fisher
information spectrum. A deeper investigation needs to be conducted on why the particular higher
order feature map used in this study produces a desirable model landscape that induces both a high
capacity, and faster training ability. Different variational circuits could also influence the model’s
landscape and the effects of non-unitary operations, induced through intermediate measurements
for example, should be investigated. Additionally, the possibility of noise-induced barren plateaus
needs examination. Finally, understanding generalisation performance on multiple datasets and
larger models will prove insightful.

10
Overall, we have shown that quantum neural networks can possess a desirable Fisher infor-
mation spectrum that enables them to train faster and express more functions than comparable
classical and quantum models—a promising reveal for quantum machine learning, which we hope
leads to further studies on the power of quantum models.

Acknowledgements We thank Maria Schuld for the insightful discussions on data embedding in
quantum models. We also thank Travis L. Scholten for constructive feedback on the manuscript and
acknowledge support from the National Centre of Competence in Research Quantum Science and
Technology (QSIT). IBM, the IBM logo, and ibm.com are trademarks of International Business
Machines Corp., registered in many jurisdictions worldwide. Other product and service names
might be trademarks of IBM or other companies. The current list of IBM trademarks is available
at https://www.ibm.com/legal/copytrade.

A Details of the quantum models

The quantum neural networks considered in this study are of the form given in Figure 1. In the
following, we explain the chosen feature maps and the variational form in more detail.

A.1 Specific feature maps

Figure 4 contains a circuit representation of the feature map developed in [39] and used in this
study in the quantum neural network model. First, the feature map applies Hadamard gates on
each of the S := si, qubits, followed by a layer of RZ-gates, whereby the angle of the Pauli rotation
on qubit i depends on the i* feature x; of the data vector Z, normalised between [-1,1].21 Then,
RZZ-gates are implemented on qubits i, i+ 9 for 2 € [1,...,S — 1] and 9g € [i+ 1,...,S] using a
decomposition into two CNOT-gates and one RZ-gate with a rotation angle (a — a;) (m — @i4;).
We consider only up to second order data encoding and the parameterised RZ and RZZ-gates are
repeated once. In other words, the feature map depth is equal to 2 and the operations after the
Hadamard gates in the circuit depicted in Figure 4 are applied again. The classically simulable
feature map employed in the easy quantum model, is simply the first sets of Hadamard and RZ-
gates, as done in Figure 4 and is not repeated.

 
 

 

Figure 4: Feature map from [39], used in the quantum neural network. First, Hadamard gates
are applied to each qubit. Then, normalised feature values of the data are encoded using RZ-gates.
This is followed by CNOT-gates and higher order data encoding between every pair of qubits, and
every pair of features in the data. The feature map is repeated to create a depth of 2. The easy
quantum model, introduced in Section 2, applies only the first sets of Hadamard and RZ-gates.

A.2 The variational form

Figure 5 depicts the variational form, deployed in both the easy quantum model and the quantum
neural network. The circuit consists of S qubits, to which parameterised RY-gates are applied.

 

21'This is to be consistent with the chosen parameter space for the classical models.

11
Thereafter, CNOT-gates are applied between every pair of qubits in the circuit. Lastly, another set
of parameterised RY-gates are applied to each qubit. This circuit has, by definition, a depth of 1
and 2S parameters. If the depth is increased, the entangling layers and second set of parameterised
RY-gates are repeated. The number of trainable parameters d can be calculated as d = (D+1)S,
where S is equal to the input size of the data sj, due to the choice of both feature maps used
in this study and D is the depth of the circuit (i.e. how many times the entanglement and RY
operations are repeated).


Figure 5: Variational circuit used in both quantum models is plotted in this figure. The circuit
contains parameterised RY-gates, followed by CNOT-gates and another set of parameterised RY-
gates.

B_ Properties of the effective dimension

B.1 Proof of Theorem 3.2

Given a positive definite matrix A > 0, and a function g : Rt — R?*, we define g{A) as the
matrix obtained by taking the image of the eigenvalues of A under the map g. In other words,
A= U'tdiag(j4,...,4a)U implies g( A) = U'diag(g(y1),...,9(4a))U. To prove the assertion of the
theorem, we start with a lemma that relates the effective dimension to the covering number.

Lemma B.1. Let @ = [—1,1]", and let N(e) denote the number of boxes of side length ¢ required
to cover the parameter set O, the length being measured with respect to the metric Fi;(@). Under
the assumption of Theorem 3.2, there exists a dimensional constant cq < co such that for y € (0, 1]

14
Remark B.3 (Improved scaling for relative entropy loss function). The relative entropy is com-
monly used as a loss function. Note that the relative entropy is log-Lipschitz in the first argument
which is better than Hélder continuous.*? As a result we can improve the bound from Lemma B.2

by following the proof given above and utilising the log-Lipschitz property of the relative entropy in
its first argument and the fact that the inverse of t|log(t)| behaves like s/|log(s)| near the origin.*+

Remark B.4 (Boundedness assumption of loss function). By utilizing a stronger concentration
bound than Hoeffding’s inequality in (10), one may be able to relax the assumption that the loss
function in Theorem 3.2 has to be bounded.

B.2 Generalisation ability of the effective dimension

In order to assess the effective dimension’s ability to capture generalisation behaviour, we conduct
a numerical experiment similar to work in [48]. Using a feedforward neural network with a single
hidden layer, an input size of si, = 6, output size soy; = 2 and number of trainable weights d = 880,
we train the network on confusion sets constructed from scikit-learn’s make blobs dataset [57].
More concretely, we use 1000 data points and train the network to zero training loss. This is
repeated several times, each time with the data labels becoming increasingly randomised, thereby
creating multiple confusion sets. The network’s size is chosen such that it is able to achieve zero
training error for all confusion sets considered.

We then calculate the effective dimension of the network, using the parameter set produced after
training on each confusion set. If a proposed capacity measure accurately captures generalisation
ability, we would expect to see an increasing capacity as the percentage of randomised labels in
the confusion set increases, until roughly 50% of the labels are randomised. A network requires
more expressive power to fit random labels (i.e. to fit noise), and this is exactly captured by the
effective dimension and plotted in Figure 6.

B.3 Effective dimension converges to maximal rank of Fisher informa-
tion matrix

The effective dimension converges to the maximal rank of the Fisher information matrix denoted
by F := maxgeo re in the limit n — oo. Since the Fisher information matrix is positive semidefinite,
it can be unitarily diagonalised. By definition of the effective dimension, we see that, without loss
of generality, F'(@) can be diagonal, ie. F(@) = diag(A1(6),...,Ar,(@),0 ...,0). Furthermore we
define the normalisation constant


normalised effective dimension


Figure 6: Generalisation behaviour of the effective dimension. We plot the normalised effective
dimension for a network trained on confusion sets with increasing randomisation, averaged over
10 different training runs, with one standard deviation above and below the mean. The effective
dimension correctly increases as the data becomes “more random” and is thus, able to accurately
capture a model’s generalisation behaviour.



This proves the other direction and concludes the proof. 

B.4 A geometric depiction of the effective dimension

The effective dimension defined in (2) does not necessarily increase monotonically with the number
of data, n. Recall that the effective dimension attempts to capture the size of a model, whilst n
determines the resolution at which the model can be observed. Figure 7 contains an intuitive
example of a case where the effective dimension is not monotone in n. We can interpret a model

16
as a geometric object. When n is small, the resolution at which we are able to see this object
is very low. In this unclear, low resolution setting, the model can appear to be a 2-dimensional
disk as depicted in Figure 7. Increasing n, increases the resolution and the model can then look
1-dimensional, as seen by the spiralling line in the medium resolution regime. Going to very
high resolution, and thus, very high n, reveals that the model is a 2-dimensional structure. In
this example, the effective dimension will be high for small n, where the model is considered 2-
dimensional, lower for slightly higher n where the model seems 1-dimensional, and high again as
the number of data becomes sufficient to accurately quantify the true model size. Similar examples
can be constructed in higher dimensions by taking the same object and allowing it to spiral inside
the unit ball of the ambient space R¢. Then, the effective dimension will be d for small n, it will
go down to a value close to 1, and finally converge to 2 as n > oo. In all experiments conducted
in this study, we examine the effective dimension over a wide range of n, to ensure it is sufficient
in accurately estimating the size of a model.

Low resolution Medium resolution High resolution

 

Figure 7: Geometric picture of a model at different resolution scales. In the low resolution
scale, the model can appear as a 2-dimensional disk and the effective dimension attempts to quantify
the size of this disk. As we enhance the resolution by increasing the number of data used in the
effective dimension, the medium scale reveals a 1-dimensional line, spiralling. Adding sufficient
data and moving to high resolution allows the effective dimension to accurately capture the model’s
true size, which in this case is actually a 2-dimensional object. Thus, the effective dimension does
not necessarily increase monotonically with the number of data used.

B.5 Removing the rank constraint via discretisation

The aim of this section is to find a suitable generalisation of the results in Section B.1 when the
Fisher information matrix does not satisfy the bound ||Vg log F’|| < A. Indeed, this is a rather
strong bound as it forces F to have constant rank, so it is desirable to find a variant of Lemmas B.1
and B.2 that do not require such an assumption.

Our approach to this general problem is based on the idea that, in practical applications, the
Fisher matrix is evaluated at finitely many points, so it makes sense to approximate a statisti-
cal model with a discretised one where the corresponding Fisher information matrix is piecewise
constant.

Let © = [-1,1]? and consider a statistical model Mo := {p(-,;9) : @ € O} with a Fisher
information matrix denoted by F(6) for 6 € @. Given an integer « > 1, we consider a discretised
version of the statistical model. More precisely, we split © into «@ disjoint cubes {Gi}, of size
2/«. Then, given one of these small cubes G;, we consider its center x; and we split G; into Q¢
disjoint simplices, where each simplex is generated by x; and one of the faces of OG;. We denote the
set of all these simplices by {O¢}7~,, where m = 24«%. Note that {Oe}7~, is a regular triangulation
of 0.


Theorem B.5 (Generalisation bound for effective dimension without rank constraint). Let @ =
[-1,1]¢ and consider a statistical model Mo := {pl-,-;9) : 6 € O} satisfying (4), For x € N,
let Me) : = {p\)(.,-;0):9€ O} be the discretised form as described above. Let ds), denote the
effective dimension of Ms) as defined in (2). Furthermore, let L : P(V) x P(Y) > [-B/2, B/2|
for B > 0 be a loss function that is a-Hélder continuous with constant M2 in the first argument
w.r.t. the total variation distance for some a € {0,1]. Then, there exists a dimensional constant
cq such that for y € (0,1) and for alln CN, we have


To prove the statement of the theorem we need a preparatory lemma that is the discretised
version of Lemma B.1.

Lemma B.6. Let 9 = {—1,1]¢, and let N“*)(e) denote the number of boxes of side len, i € required
to cover the parameter set Q, the length being measured with respect to the metric BAS (@). Under
the assumption of Theorem B.5, there exists a dimensional constant cq < co such that vat y € (0, 1]
and for alln EN, we have

Proof. Recall that we work in the discretised model ME ), so our metric FAS) (@) is constant on
each element ©, of the partition. So, we fix 2, and we count first the number of boxes of side length
€ required to cover Og.

Up to a rotation, we can diagonalise the Fisher information matrix fF“) le, as diag(s
Note that @, has Euclidean diameter bounded by 2«~! and volume «~¢. Also, if B.(8e
centered at 6 € Og and of length ¢, then

 

Remark B.7 (How to choose the discretisation parameter «). In this remark we discuss conditions
such that the generalisation bound of Theorem B.5 for the discretised model Me) is a good
approximation to a generalisation bound of the original model Mo. Assume that the model Mo
satisfies an additional regularity assumption of the form ||Vo£()|| < A for some A > 0 and for
all 6 € ©, then choosing the discretisation parameter « >> A ensures that that Mo = Me and
F(6) = F(6). Furthermore, “7 >> « is required to ensure that the balls used to cover each
simplex of the triangulation are smaller than the size of each simplex.

C Numerical experiments

C.1 Sensitivity analysis for the effective dimension

We use Monte Carlo sampling to estimate the effective dimension. The capacity results will, thus,
be sensitive to the number of data samples used in estimating the Fisher information matrix for a
given @, and to the number of @ samples then used to calculate the effective dimension. We plot
the normalised effective dimension with n fixed, in Figure 8 over an increasing number of data
and parameter samples using the classical feedforward model. For networks with less trainable
parameters, d, the results stabilise with as little as 40 data and parameter samples. When higher
dimensions are considered, the standard deviation around the results increases, but 100 data and
parameter samples are still reasonable given that we consider a maximum of d = 100. For higher
d, it is likely that more samples will be needed.

C.2 The Fisher information spectra for varying model size

Figure 9 plots the average distribution of the Fisher information eigenvalues for all model types,
over increasing input size, sj,, and hence, increasing number of parameters, d. These average
distributions are generated using 100 Fisher information matrices with parameters, #, drawn uni-
formly at random on 9 = [—1, 1)“. Row A contains the histograms for models with sin = 6, row
B for si, = 8 and row C for sj, = 10. In all scenarios, the classical model has a majority of its
eigenvalues near or equal to zero, with a few very large eigenvalues. The easy quantum model
has a somewhat uniform spectrum for a smaller input size, but this deteriorates as the input size
{also equal to the number of qubits in this particular model) increases. The quantum neural net-
work, however, maintains a more uniform spectrum over increasing sj, and d, showing promise in
avoiding unfavourable qualities, such as barren plateaus.

 
 
number of data and parameter samples used

Figure 8: Sensitivity analysis of the normalised effective dimension to different numbers of
data and parameter samples, used in calculating the empirical Fisher information matrix, and
subsequently, the effective dimension.

C.3 Training the models using a simulator

To test the trainability of all three model types, we conduct a simple experiment using the Iris
dataset. In each model, we use an input size of si, = 4, output size soy; = 2 and d = 8 trainable
parameters. We train the models for 100 training iterations, using 100 data points from the first
two classes of the dataset. Standard hyperparameter choices are made, using an initial learning
rate = 0.1 and the ADAM optimiser. Each model is trained 100 times, with initial parameters @
sampled uniformly on 6 = [—1, 1]¢ each trial.?® The average training loss and average Fisher-Rao
norm after 100 training iterations, is captured in Table 1. The quantum neural network notably
has the highest Fisher-Rao norm and lowest training loss on average.

 

We choose 9 = [-1, 1]? as the sample space for the initial parameters, as well as for the parameter sample space
in the effective dimension. Another convention is to use [—27, Qn]? as the parameter space for initialisation of the
quantum model, however, we stick with [-1, 1]? to be consistent and align with classical neural network literature.
We note that for the effective dimension, using either parameter space does affect the observed results.

C.4 Training the quantum neural network on real hardware

The hardware experiment is conducted on the ibmqmontreal 27-qubit device. We use 4 qubits
with linear connectivity to train the quantum neural network on the first two classes of the Iris
dataset. We deploy the same training specifications as in Appendix ©.3 and randomly initialise
the parameters. Once the training loss stabilises, i.e. the change in the loss from one iteration to
the next is small, we stop the hardware training. This occurs after roughly 33 training steps. The
results are contained in Figure 3b and the real hardware shows remarkable performance relative
to all other models. Due to limited hardware availability, this experiment is only run once and an
analysis of the hardware noise and the spread of the training loss for differently sampled initial
parameters would make these results more robust.

We plot the circuit that is implemented on the quantum device in Figure 10. As in the quantum
neural network discussed in Appendix A, the circuit contains parameterised RZ and RZZ rotations
that depend on the data, as well as parameterised RY-gates with 8 trainable parameters. Note
the different entanglement structure presented here as opposed to the circuits in Figures 4 and 5.
This is to reduce the number of CNOT-gates required, in order to incorporate current hardware
