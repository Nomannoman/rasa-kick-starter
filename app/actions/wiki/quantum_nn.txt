2011.00027v1 [quant-ph] 30 Oct 2020

arX1Vv

The power of quantum neural networks

Amira Abbas!?, David Sutter!, Christa Zoufal'?, Aurelien Lucchi?,
Alessio Figalli?, and Stefan Woerner?*

‘TBM Quantum, IBM Research — Zurich
? University of KwaZulu-Natal, Durban
SETH Zurich

Abstract

Fault-tolerant quantum computers offer the promise of dramatically improving machine
learning through speed-ups in computation or improved model scalability. In the near-term,
however, the benefits of quantum machine learning are not so clear. Understanding expressibil-
ity and trainability of quantum models—and quantum neural networks in particular—requires
further investigation. In this work, we use tools from information geometry to define a notion
of expressibility for quantum and classical models. The effective dimension, which depends
on the Fisher information, is used to prove a novel generalisation bound and establish a ro-
bust measure of expressibility. We show that quantum neural networks are able to achieve
a significantly better effective dimension than comparable classical neural networks. To then
assess the trainability of quantum models, we connect the Fisher information spectrum to
barren plateaus, the problem of vanishing gradients. Importantly, certain quantum neural
networks can show resilience to this phenomenon and train faster than classical models due to
their favourable optimisation landscapes, captured by a more evenly spread Fisher information
spectrum. Our work is the first to demonstrate that well-designed quantum neural networks
offer an advantage over classical neural networks through a higher effective dimension and
faster training ability, which we verify on real quantum hardware.

1 Introduction

The power of a model lies in its ability to fit a variety of functions [1]. In machine learning, power
is often referred to as a model’s capacity to express different relationships between variables [2].
Deep neural networks have proven to be extremely powerful models, capable of capturing intricate
relationships by learning from data [3]. Quantum neural networks serve as a newer class of machine
learning models that are deployed on quantum computers and use quantum effects such as super-
position, entanglement, and interference, to do computation. Some proposals for quantum neural
networks include [4-11] and hint at potential advantages, such as speed-ups in training and faster
processing. Whilst there has been much development in the growing field of quantum machine
learning, a systematic study of the trade-offs between quantum and classical models has yet to be
conducted [12]. In particular, the question of whether quantum neural networks are more powerful
than classical neural networks, is still open.

A common way to quantify the power of a model is by its complexity [13]. In statistical learn-
ing theory, the Vapnik-Chervonenkis (VC) dimension is an established complexity measure, where
error bounds on how well a model generalises (i.e., performs on unseen data), can be derived [14].
Although the VC dimension has attractive properties in theory, computing it in practice is noto-
riously difficult. Further, using the VC dimension to bound generalisation error requires several
unrealistic assumptions, including that the model has access to infinite data [15,16]. The measure
also scales with the number of parameters in the model and ignores the distribution of data. Since
modern deep neural networks are heavily overparameterised, generalisation bounds based on the
VC dimension, and other measures alike, are typically vacuous [17, 18].

 

*wor@zurich.ibm.com
In [19], the authors analysed the expressive power of parameterised quantum circuits using
memory capacity, and found that quantum neural networks had limited advantages over classical
neural networks. Memory capacity is, however, closely related to the VC dimension and is thus,
subject to similar criticisms.

We therefore, turn our attention to measures that are calculable in practice and incorporate the
distribution of data. In particular, measures such as the effective dimension have been motivated
from an information-theoretic standpoint and depend on the Fisher information; a quantity that
describes the geometry of a model’s parameter space and is essential in both statistics and machine
learning [20-22]. We argue that the effective dimension is a robust capacity measure through proof
of a novel generalisation bound with supporting numerical analyses, and use this measure as a tool
to study the power of quantum and classical neural networks.

Despite a lack of quantitative statements on the power of quantum neural networks, another
issue is rooted in the trainability of these models. Often, quantum neural networks suffer from
a barren plateau phenomenon, wherein the loss landscape is perilously flat, and consequently,
parameter optimisation is extremely difficult [23]. As shown in [24], barren plateaus may be noise-
induced, where certain noise models are assumed on the hardware. On the other hand, noise-free
barren plateaus are circuit-induced, which relates to random parameter initialisation, and methods
to avoid them have been explored in [25-28].

A particular attempt to understand the loss landscape of quantum models uses the Hessian
in [29]. The Hessian quantifies the curvature of a model’s loss function at a point in its parameter
space [30]. Properties of the Hessian matrix, such as its spectrum, provide useful diagnostic
information about the trainability of a model [31]. It was also discovered that the entries of
the Hessian, vanish exponentially in models suffering from a barren plateau [32]. For certain
loss functions, the Fisher information matrix coincides with the Hessian of the loss function [33).
Consequently, we examine the trainability of quantum and classical neural networks by analysing
the Fisher information matrix, which is incorporated by the effective dimension. In this way, we
can explicitly relate the effective dimension to model trainability [34].

We find that well-designed quantum neural networks are able to achieve a higher capacity and
faster training ability than comparable classical feedforward neural networks.! Capacity is captured
by the effective dimension, whilst trainability is assessed by leveraging the information-theoretic
properties of the Fisher information. Lastly, we connect the Fisher information spectrum to the
barren plateau phenomenon and find that a quantum neural network with an easier data encoding
strategy, increases the likelihood of encountering a barren plateau, whilst a harder data encoding
strategy shows resilience to the phenomenon.” The remainder of this work is organised as follows.
In Section 2, we discuss the types of models used in this study. Section 3 introduces the effective
dimension from [20] and motivates its relevance as a capacity measure by proving a generalisation
bound. We additionally relate the Fisher information spectrum to model trainability in Section 3.
This link, as well as the power of quantum and classical models, is analysed through numerical
experiments in Section 4, where the training results are further supported by an implementation
on the ibmq-montreal 27-qubit device.

2 Quantum neural networks

Quantum neural networks are a subclass of variational quantum algorithms, comprising of quan-
tum circuits that contain parameterised gate operations [35]. Information is first encoded into a
quantum state via a state preparation routine or feature map [36]. The choice of feature map is
usually geared toward enhancing the performance of the quantum model and is typically neither
optimised nor trained, though this idea was discussed in [37]. Once data is encoded into a quantum
state, a variational model containing parameterised gates is applied and optimised for a particular
task [5-7,38]. This happens through loss function minimisation, where the output of a quantum
model can be extracted from a classical post-processing function that is applied to a measurement
outcome.

 

Paster training implies a model will reach a lower training error than another comparable model for a fixed num-
ber of training iterations. We deem two models comparable if they share the same number of trainable parameters
and the same input and output size.

?Easy and hard refer to the ability of a classical computer to simulate the particular data encoding strategy.
 
   

\9}
> 2
2 Gale) = lao) ant:
8 = |go\e :
7 Z : ° 5 ¥
: id
> 0) i>
feature map variational model measurement

Uy Ge f@=y

Figure 1: Overview of the quantum neural network used in this study. The input « € R*=
is encoded into an S-qubit Hilbert space by applying the feature map |7z) := Uz jaye", This
state is then evolved via a variational form |ge{x)) := Go |Wz), where the parameters 6 € © are
chosen to minimise a certain loss function. Finally a measurement is performed whose outcome
z =(z1,...,2g) is post-processed to extract the output of the model y := f(z).

The model we use is depicted in Figure 1. It encodes classical data « € R*™ into an S-qubit
Hilbert space using the feature map U/, proposed in [39]. First, Hadamard gates are applied to each
qubit. Then, normalised feature values of the data are encoded using RZ-gates with rotation angles
equal to the feature values of the data. This is then accompanied by RZZ-gates that encode higher
orders of the data, i.e. the controlled rotation values depend on the product of feature values. The
RZ and RZZ-gates are then repeated.? Once data is encoded, the model optimises a variational
circuit Gg containing parameterised RY-gates with CNOT entangling layers between every pair
of qubits, where 6 € © denotes the trainable parameters. The post-processing step measures all
qubits in the o, basis and classically computes the parity of the output bit strings. For simplicity,
we consider binary classification, where the probability of observing class 0 corresponds to the
probability of seeing even parity and similarly, for class 1 with odd parity. The reason for the
choice of this model architecture is two-fold: the feature map is motivated in [39] to serve as a
useful data embedding strategy that is believed to be difficult to simulate classically as the depth
and width increase*, which we find adds substantial power to a model (as seen in Section 4.2);
and the variational form aims to create more expressive circuits for quantum algorithms [40].
Detailed information about the circuit implementing the quantum neural network is contained in
Appendix A.

We benchmark this quantum neural network against classical feedforward neural networks with
full connectivity and consider all topologies for a fixed number of trainable parameters.” We also
adjust the feature map of the quantum neural network to investigate how data encoding impacts
capacity and trainability. We use a simple feature map that is easy to reproduce classically and
thus, refer to it as an easy quantum model.®

3 Information geometry, effective dimension, and trainabil-
ity of quantum neural networks

We approach the notion of complexity from an information geometry perspective. In doing so,

we are able to rigorously define measures that apply to both classical and quantum models, and
subsequently use them to study the capacity and trainability of neural networks.

 

3In general, these encoding operations can be repeated by an arbitrary amount. The amount of repetitions is
termed the depth of the feature map.

4This is conjectured to be difficult for depth > 2.

5Networks with and without biases and different activation functions are explored. In particular, RELU, leaky
RELU, tanh and sigmoid activations are considered. We keep the number of hidden layers and neurons per layer
variable and initialise with random weights sampled from [—1, 1]?.

®We use a straightforward angle encoding scheme, where data points are encoded via RY-gates on each qubit
without entangling them, with rotations equal to feature values normalised to [—1,1]. See Appendix A for further
details.
3.1 The Fisher information

The Fisher information presents itself as a foundational quantity in a variety of fields, from physics
to computational neuroscience [41]. It plays a fundamental role in complexity from both a com-
putational and statistical perspective [21]. In computational learning theory, it is used to measure
complexity according to the principle of minimum description length [42]. We focus on a statistical
interpretation, which is synonymous with model capacity: a quantification of the class of functions
a model can fit [1].

A way to assess the information gained by a particular parameterisation of a statistical model
is epitomised by the Fisher information. By defining a neural network as a statistical model, we
can describe the joint relationship between data pairs (2, y) as p(z,y;6) = p(y|x;@)p(2) for all
2eX¥ CR, ye YC R™* and @€ OC [-1,1]*%.” The input distribution, p(x) is a prior
distribution and the conditional distribution, p{y|x;@) describes the input-output relation of the
model for a fixed @ € @. The full parameter space 0 forms a Riemannian space which gives rise
to a Riemannian metric, namely, the Fisher information matrix

a a T dxd
F(6) = Eve y)~p Fe log pla, y5 ®) a5 logp(z. 456) | ERO,
that can be approximated by the empirical Fisher information matrix

k
ly? a py2 apy
Fel) = 5D ag OB Plas wi) og log p(xj, 453) , (1)

where (xj,us)F 4 are iid. drawn from the distribution p(a, y;@) [33].8 By definition, the Fisher
information matrix is positive semidefinite and hence, its eigenvalues are non-negative, real num-
bers.

The Fisher information conveniently helps capture the sensitivity of a neural network’s output
relative to movements in the parameter space, proving useful in natural gradient optimisation—a
method that uses the Fisher information as a guide to optimally navigate through the parameter
space such that a model’s loss declines [43]. In [44], the authors leverage geometric invariances
associated with the Fisher information, to produce the Fisher-Rao norm-a robust norm-based
capacity measure, defined as the quadratic form \| lle. := 6'F (6) for a vectorised parameter
set, 8. Notably, the Fisher-Rao norm acts as an umbrella for several other existing norm-based
measures [45-47] and has demonstrated desirable properties both theoretically, and empirically.

3.2. The effective dimension

The effective dimension is an alternative complexity measure motivated by information geometry,
with useful qualities. The goal of the effective dimension is to estimate the size that a model
occupies in model space-the space of all possible functions for a particular model class, where the
Fisher information matrix serves as the metric. Whilst there are many ways to define the effective
dimension, a useful definition which we apply to both classical and quantum models is presented
in [20]. The number of data observations determines a natural scale or resolution used to observe
model space. This is beneficial for practical reasons where data is often limited, and can help in
understanding how data availability influences the accurate capture of model complexity.

Definition 3.1. The effective dimension of a statistical model Mo := {p(-,-;9) : 9 € O} with
respect to y € (0,1), a d-dimensional parameter space @ C R? and n €N, n > 1 data samples is
defined as

log (te fy [det (ida + = P(O)) as)

log ( Sarton n )

‘This is achieved by applying an appropriate post-processing function in both classical and quantum networks. In
the classical network, we apply a softmax function to the last layer. In the quantum network, we obtain probabilities
based on the post-processing parity function. Both techniques are standard in practice.

8It is important that (aj .45 yey are drawn from the true distribution p(x, y; 8) in order for the empirical Fisher
information to approximate the Fisher information, i.e., img. Fx (8) = F(9) [33]. This is ensured in our numerical
analysis by design.

 

dyn(Me) := 2 (2)

 
where Vg := f{,d@ € Ry is the volume of the parameter space. F(0) € R®¢ is the normalised
Fisher information matrix defined as

i; —_— Vo we

where the normalisation ensures that ve to tr(F(0))d@ = d.

The effective dimension neatly incorporates the Fisher information spectrum by integrating
over its determinant. There are two minor differences between (2) and the effective dimension
from [20]: the presence of the constant y € (0,1], and the logn term. These modifications are
helpful in proving a generalisation bound, such that the effective dimension can be interpreted as
a bounded capacity measure that serves as a useful tool to analyse the power of statistical models.
We demonstrate this in the following section.

3.3 Generalisation error bounds

Suppose we are given a hypothesis class, H, of functions mapping from ¥V to Y and a training
set Sy = {(21,y1),---, (fn, Yn)} © (¥ x Y)”, where the pairs (2;,y;) are drawn iid. from some
unknown joint distribution p. Furthermore, let D : Y x Y > R be a loss function. The chal-
lenge is to find a particular hypothesis h € H with the smallest possible expected risk, defined
as R(h) := Evey)~pl|L(h(a),y)]. Since we only have access to a training set S,, a good strat-
egy to find the best hypothesis h € H is to minimise the so called empirical risk, defined as
Rn(h) = + Ty L(h(z:), yi). The difference between the expected and the empirical risk is the
generalisation error—an important quantity in machine learning that dictates whether a hypothesis
h €# learned on a training set will perform well on unseen data, drawn from the unknown joint

distribution p [17|. Therefore, an upper bound on the quantity
sup |R(h) — Ra(h)|, (3)
hEH.

which vanishes as n grows large, is of considerable interest. Capacity measures help quantify the
expressiveness and power of #. Thus, the generalisation error in (3) is typically bounded by an
expression that depends on a capacity measure, such as the VC dimension [3] or the Fisher-Rao
norm [44]. Theorem 3.2 provides a novel bound based on the effective dimension, which we use to
study the power of neural networks from hereon.

Bounding generalisation error with the effective dimension In this manuscript, we con-
sider neural networks as models described by stochastic maps, parameterised by some 6 € @.°
The corresponding loss functions are mappings L : P{) x P(Y) — R, where P()) denotes
the set of distributions on Y. We assume the following regularity assumption on the model

Me == {pl-, 30): 6 € O}:
056+ p(-,-;6) is M\-Lipschitz continuous w.r.t. the supremum norm . (4)

Theorem 3.2 (Generalisation bound for the effective dimension). Let 9 = [—1,1]¢ and consider
a statistical model Me := {p(-,-30) : 6 € O} satisfying (4) such that the normalised Fisher
information matrix F(0) has full rank for all 6 € ©, and ||Volog F(9)\| <A for some A > 0 and
all @ € ©. Let dy denote the effective dimension of Me as defined in (2). Furthermore, let
E:P() x P(Y) — [-B/2,B/2] for B > 0 be a loss function that is a-Hélder continuous with
constant M in the first argument w.r.t. the total variation distance for some a € (0,1]. Then
there exists a constant ca, such that for y € (0,1] and alln EN, we have

d

27 logn ynile = 16M ?nlogn
P { sup |R(@) — R,(6)| > 4M, | ——— |] <caa () exp (-“*) , (5)
9c yn Bey

where M = Mf'Mo.

 

°As a result, the variables h and H are replaced by @ and 9, respectively.
The proof is given in Appendix B.1. Note that the choice of the norm to bound the gradient
of the Fisher information matrix is irrelevant due to the presence of the dimensional constant
Can? If we choose y € (0,1] to be sufficiently small, we can ensure that the right-hand side
of (5) vanishes in the limit n > oo.1! To verify the effective dimension’s ability to capture
generalisation behaviour, we conduct a numerical analysis similar to work presented in [48]. We find
that the effective dimension for a model trained on confusion sets with increasing label corruption,
accurately captures generalisation behaviour. The details can be found in Appendix B.2.

Remark 3.3 (Properties of the effective dimension). In the limit m — oo, the effective dimension
converges to the maximal rank 7 := maxgeo@rg, where rg < d denotes the rank of the Fisher
information matrix F'(@). The proof of this result can be seen in Appendix B.3, but it is worthwhile
to note that the effective dimension does not necessarily increase monotonically with n, as explained
in Appendix B.4.!?

The continuity assumptions of Theorem 3.2 are satisfied for a large class of classical and quan-
tum statistical models [49,50], as well as many popular loss functions. The full rank assumption
on the Fisher information matrix, however, often does not hold in classical models. Non-linear
feedforward neural networks, which we consider in this study, have particularly degenerate Fisher
information matrices [34]. Thus, we further extend the generalisation bound to account for a broad
range of models that may not have a full rank Fisher information matrix.

Remark 3.4 (Relaxing the rank constraint in Theorem 3.2). The generalisation bound in (5) can
be modified to hold for a statistical model without a full rank Fisher information matrix. By parti-
tioning the parameter space O, we discretise the statistical model and prove a generalisation bound
for the discretised version of Me := {p(-,-;6) : 6 € O} denoted by MSs) = {[p')(,-0) 26 € OF,
where « € Nis adiscretisation parameter. By choosing « carefully, we can control the discretisation
error. This is explained in detail, along with the proof, in Appendix B.5.

3.4 The Fisher spectrum and the barren plateau phenomenon

The Fisher information spectrum for fully connected feedforward neural networks reveals that the
parameter space is flat in most dimensions, and strongly distorted in a few others [34]. These
distortions are captured by a few very large eigenvalues, whilst the flatness corresponds to eigen-
values being close to zero. This behaviour has also been reported for the Hessian matrix, which
coincides with the Fisher information matrix under certain conditions [33,51,52].1° These types of
spectra are known to slow down a model’s training and may render optimisation suboptimal [31].
In the quantum realm, the negative effect of barren plateaus on training quantum neural networks
has been linked to the Hessian matrix [32]. It was found that the entries of the Hessian vanish
exponentially with the size of the system in models that are in a barren plateau. This implies that
the loss landscape becomes increasingly flat as the size of the model increases, making optimisation
more difficult.

The Fisher information can also be connected to barren plateaus. Assuming a log-likelihood
loss function, without loss of generality, we can formulate the empirical risk over the full training

 

10In the special case where the Fisher information matrix does not depend on 6, we have A = 0 and (5) holds for
C40 = 2J/d. This may occur in scenarios where a neural network is already trained, i.e., the parameters 6 € © are
fixed.

1lMore precisely, this occurs if y scales at most as y ~ 327aM?/(dB?). To see this, we use the fact that
dyn <d+7/|logn| for some constant Tt > 0.

!2The geometric operational interpretation of the effective dimension only holds if n is sufficiently large. We
conduct experiments over a wide range of n and ensure that conclusions are drawn from results where the choice of
n is sufficient.

13For example, under the use of certain loss functions.
set as
1 ” i< la
Ra(8) = ——Iog ( J] plyiless8)) = —— Slog plyrlais8),
i=l i=l

where p(y;|2:;8) is the conditional distribution for a data pair (x;,y;).1° From Bayes rule, note
that the derivative of the empirical risk function is then equal to the derivative of the log of the
joint distribution summed over all data pairs, i-e.,

a alc 1d
5 Rn(8) = a5 Dos (yl 8) =-= 57 bsnl i, yis8) = —— » 5p log (m, 4139)

since the prior distribution p(-) does not depend on 6. From [23], we know that we are in a barren
plateau if, for parameters @ uniformly sampled from ©, each element of the gradient of the loss
function with respect to @ vanishes exponentially in the number of qubits, S. In mathematical
terms this means

n

 

 

 

0 1a a 1 8
wm -|oo[23 2 orsuacn =| 9m 2 aeons son
| 6 ay Fo) 6 no BB; og p(2i, yi3 9) no 6 56, og p(ti, yi ®)| | < ws
for all 7 = 1,...d and for some nonnegative constant ws that goes to zero exponentially fast with

increasing S. The barren plateau result also tells us that Vare[s3- Rr, (6)] < wg for models in a

barren plateau. By definition of the empirical Fisher information in (1), the entries of the Fisher
matrix can be written as

o o

6)——R,,(8) ,
pan)

for j,k =1,...,d. Hence we can write

O

Eo[F(@);;] = Eo (cel). = Varg aan) + (= [Zrwo)]) <wg +we,

which implies tr(Ee[F(@)|) < d(ws +w%). Due to the positive semidefinite nature of the Fisher
information matrix and by definition of the Hilbert-Schimdt norm, all matrix entries will approach
zero if a model is in a barren plateau, and natural gradient optimisation techniques become unfea-
sible. We can conclude that a model suffering from a barren plateau will have a Fisher information
spectrum with an increasing concentration of eigenvalues approaching zero as the number of qubits
in the model increase. Conversely, a model with a Fisher information spectrum that is not con-
centrated around zero is unlikely to experience a barren plateau.

We investigate the spectra of quantum and classical neural networks in the following section
and verify the trainability of these models with numerical experiments, including results from real
quantum hardware.

 

4 Numerical experiments and results

In this section, we compare the Fisher information spectrum, effective dimension and training
performance of the quantum neural network to feedforward models with different topologies. We
also include the easy quantum model with a classically simulable feature map to understand the
impact of data encoding on model expressibility and trainability. Trainability is further verified

 

14Minimising the empirical risk with a log-likelihood loss function coincides with the task of minimising the relative
entropy D(-||-} between the distribution induced by applying the neural network to the observed input distribution
r and the observed output distribution g. Hence, equivalent to the log-likelihood loss function, we can choose
L(p(ylz; r(x), av) = D(e(y|2; 9)r(z)||a(y)), which fits the framework presented in Section 3.3. We further note
that the relative entropy is a-Hélder continuous in the first argument for a € (0,1). In fact, the relative entropy
is even log-Lipschitz continuous in the first argument, which can be utilised to strengthen the generalisation bound
from Theorem 3.2 as explained in Remark B.3.

15 As is the case with the parity function chosen in the quantum neural network, and the softmax function chosen
in the last layer of the classical neural network.
for the quantum neural network on the ibmq-montreal 27-qubit device available through the
IBM Quantum Experience via Qiskit [53]. In order to do a systematic study, we deem two models

comparable if they share the same number of trainable parameters (d), input size (s;,), and output
size (Sour), and consider d < 100, sin € {4,6, 8,10} with sou; = 2.

4.1 The Fisher information spectrum

Strong connections to capacity and trainability can be derived from the spectrum of the Fisher
information matrix. For each model with a specified triple (d, sin, Sout), we sample 100 sets of
parameters uniformly on © = [—1, 1? and compute the Fisher information matrix 100 times using
a standard Gaussian prior.'© The resulting average distributions of the eigenvalues of these 100
matrices are plotted in Figure 2 for d = 40, sin) = 4 and say = 2.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

classical neural network easy quantum model quantum neural network
1.0 1.0 1.0
10 16 20
2 os os 0.8 4 os a8 4 as
a
a
8 08 oe ‘
yg 0.64 0.64 0.6 4
oD
3B os oa
3
044 02 044 on4 044 aa
3
4
oa , oa » + —
ved a T 2 : v2 4 nan 025 9.50 0.75 100 128 Tea] yy | om 02 at ob Ok 1a
0.0 T T T T 0.0 “7 T T T 0.0 — T T T T
0 5 10 15 20 0 2 4 6 0 1 2 3 4

eigenvalue size (bins 5)

Figure 2: Average Fisher information spectrum plotted as a histogram for the classical
feedforward neural network and the quantum neural network with two different feature maps.
The plot labelled easy quantum model has a classically simulable data encoding strategy, whilst
the quantum neural network’s encoding scheme is conjectured to be difficult. In each model, we
compute the Fisher information matrix 100 times using parameters sampled uniformly at random.
We fix the number of trainable parameters d = 40, input size sj, = 4 and output size sou, = 2. The
distribution of eigenvalues is the most uniform in the quantum neural network, whereas the other
models contain mostly small eigenvalues and larger condition numbers. This is made more evident
by plotting the distribution of eigenvalues from the first bin in subplots within each histogram
plot.

The classical model’s Fisher information spectrum is concentrated around zero, where the ma-
jority of eigenvalues are negligible!’ , however, there are a few very large eigenvalues. This behaviour
is observed across all classical network configurations that we consider.'* This is consistent with
results from literature, where the Fisher information matrix of non-linear classical neural networks
is known to be highly degenerate, with a few large eigenvalues [34]. The concentration around
zero becomes more evident in the subplot contained in each histogram depicting the eigenvalue
distribution of the first bin. The easy quantum model also has most of its eigenvalues close to zero,
and whilst there are some large eigenvalues, their magnitudes are not as extreme as the classical
model. The quantum neural network, on the other hand, has a different Fisher information spec-
trum. The distribution of eigenvalues is more uniform, with no outlying values and remains more
or less constant as the number of qubits increase (see Appendix C.2). This can be seen from the
range of the eigenvalues on the x-axis in Figure 2 and has implications for capacity and trainability
which we examine next.

 

16A sensitivity analysis is included in Appendix C.1 to verify that 100 parameter samples are reasonable for the
models we consider. In higher dimensions, this number will need to increase.

Specifically, of the order 10~ "4, i.e., close to machine precision and thus, indistinguishable from zero.

18The classical model depicted in Figure 2 is the one with the highest average rank of Fisher information matrices
from all possible classical configurations for a fixed number of trainable parameters, which subsequently gives rise
to the highest effective dimension.
4.2. Capacity analysis

The quantum neural network consistently achieves the highest effective dimension over all ranges of
finite data we consider.'° The reason is due to the speed of convergence, which is slowed down by
smaller eigenvalues and an unevenly distributed Fisher information spectrum. Since the classical
models contain highly degenerate Fisher information matrices, the effective dimension converges
the slowest, followed by the easy quantum model. The quantum neural network, on the other
hand, has a non-degenerate Fisher information matrix and the effective dimension converges to
the maximum effective dimension, d.?° It also converges much faster due to its more evenly spread
Fisher information spectrum. In Figure 3a, we plot the normalised effective dimension for all three

models. The normalisation ensures that the effective dimension lies between 0 and 1 by simply
dividing by d.

 

 

    

 

 

 

 

 

 

 

oO .
‘ o.8 classical neural network
os easy quantum model
5 09 7 quantum neural network
2 — ibmq_montrcal backend
® © os
£ os = a
3 &
>
©
3 goon
3 07 &
eS
vo Od
og
®
QZ 06
“s 0.3
&
a
2 —_—
2 os vol rrr nnn SSS ESSE SESS
oo 02 oa 8 os an a 20 40 60 80 100
number of data number of iterations
(a) (b)

Figure 3: (a) Normalised effective dimension plotted for the quantum neural network in
green, the easy quantum model in blue and the classical feedforward neural network in red. We
fix the input size si, = 4, the output size $44; = 2 and number of trainable parameters d = 40.
Notably, the quantum neural network achieves the highest effective dimension over a wide range
of data availability, followed by the easy quantum model. The classical model never achieves an
effective dimension greater or equal to the quantum models for the range of finite data considered
in this study. (b) Training loss. Using the first two classes of the Iris dataset [54], we train
all three models using d = 8 trainable parameters with full batch size. The ADAM optimiser with
an initial learning rate of 0.1 is selected. For a fixed number of training iterations = 100, we train
all models over 100 trials and plot the average training loss along with +1 standard deviation.
The quantum neural network maintains the lowest loss value on average across all three models,
with the lowest spread over all training iterations. Whilst on average, the classical model trains
to a lower loss than the easy quantum model, the spread is significantly larger. We further verify
the performance of the quantum neural network on real quantum hardware and train the model
using the ibmq-montreal 27-qubit device, where the training advantage persists. We plot the
hardware results till they stabilise, at roughly 33 training iterations and find the performance to
be even better than the simulated results.

The quantum neural network outperforms both models, followed by the easy quantum model
and lastly, the classical model. Capacity calculations using the Fisher-Rao norm confirm these
trends. The average Fisher-Rao norm over 100 trials is roughly 250% higher in the quantum
neural network than in the classical neural network, after training the models on a simple dataset
for a fixed number of iterations (see Appendix C.3 for details).

 

19Tn the limit n > oo, all models will converge to an effective dimension equal to the maximum rank of the Fisher
information matrix.
20See Remark 3.3.
4.3 Trainability

Upon examining the quantum neural network over an increasing system size (see Appendix C.2),
the eigenvalue distribution of the Fisher information matrix remains more or less constant, and a
large amount of the eigenvalues are not near zero, thus, the model shows resilience against barren
plateaus. This is not the case in the easy quantum model. The Fisher information spectrum be-
comes more “barren plateau-like”, with the eigenvalues becoming smaller as the number of qubits
increase. This highlights the importance of the feature map which can influence the likelihood of
experiencing a barren plateau. The higher order feature map used in the quantum neural network
seems to structurally change the optimisation landscape and remove the flatness, usually associ-
ated with barren plateaus or suboptimal optimisation conditions. Classically, the observed Fisher
information spectrum is known to have undesirable optimisation properties where the outlying
eigenvalues slow down training and loss convergence [31].

We confirm the training statements for all three models with an experiment illustrated in
Figure 3b. Using a cross-entropy loss function, optimised with ADAM for a fixed number of training
iterations = 100 and an initial learning rate = 0.1, the quantum neural network trains to a lower
loss, faster than the other two models over an average of 100 trials. To support the promising
training performance of the quantum neural network, we also train it once on real hardware using
the ibmq-montreal 27-qubit device. We reduce the number of CNOT-gates by only considering
linear entanglement instead of all-to-all entanglement in the feature map and variational circuit.
This is to cope with hardware limitations. The full details of the experiment are contained in
Appendix C.4. We find that the quantum neural network is capable of performing even better on
real hardware, thus, tangibly demonstrating faster training.

5 Conclusion

In stark contrast to classical models, understanding the capacity of quantum neural networks is not
well explored. Moreover, classical neural networks are known to produce highly degenerate Fisher
information matrices, which can significantly slow down training. For quantum neural networks,
no such analysis has been done.

In this study, the effective dimension is presented as a robust capacity measure for quantum
and classical models, which we justify through proof of a novel generalisation bound. A particular
quantum neural network offers advantages from both a capacity and trainability perspective. These
advantages are captured by a high effective dimension and a non-degenerate Fisher information
matrix. The feature map in the quantum neural network is conjectured to be hard to simulate
classically, and replacing it with one that is easily simulable, impairs these advantages. This
illustrates the importance of the choice of feature map in designing a powerful quantum neural
network that is able to train well.

Regarding quantum model trainability, the Fisher information spectrum informs us of the
likelihood of experiencing a barren plateau. Changing the feature map, influences the Fisher
spectrum and hence, alters the likelihood of encountering a barren plateau. Again, this points to
the significance of the feature map in a quantum neural network. A model with eigenvalues of the
Fisher information matrix that do not vanish as the number of qubits grow, is unlikely to suffer
from a barren plateau. The quantum neural network with a hard feature map is an example of
such a model showing resilience to this phenomenon with good trainability, supported by results
from real quantum hardware.

This work opens many doors for further research. The feature map in a quantum model plays
a large role in determining both its capacity and trainability via the effective dimension and Fisher
information spectrum. A deeper investigation needs to be conducted on why the particular higher
order feature map used in this study produces a desirable model landscape that induces both a high
capacity, and faster training ability. Different variational circuits could also influence the model’s
landscape and the effects of non-unitary operations, induced through intermediate measurements
for example, should be investigated. Additionally, the possibility of noise-induced barren plateaus
needs examination. Finally, understanding generalisation performance on multiple datasets and
larger models will prove insightful.

10
Overall, we have shown that quantum neural networks can possess a desirable Fisher infor-
mation spectrum that enables them to train faster and express more functions than comparable
classical and quantum models—a promising reveal for quantum machine learning, which we hope
leads to further studies on the power of quantum models.

Acknowledgements We thank Maria Schuld for the insightful discussions on data embedding in
quantum models. We also thank Travis L. Scholten for constructive feedback on the manuscript and
acknowledge support from the National Centre of Competence in Research Quantum Science and
Technology (QSIT). IBM, the IBM logo, and ibm.com are trademarks of International Business
Machines Corp., registered in many jurisdictions worldwide. Other product and service names
might be trademarks of IBM or other companies. The current list of IBM trademarks is available
at https://www.ibm.com/legal/copytrade.

A Details of the quantum models

The quantum neural networks considered in this study are of the form given in Figure 1. In the
following, we explain the chosen feature maps and the variational form in more detail.

A.1 Specific feature maps

Figure 4 contains a circuit representation of the feature map developed in [39] and used in this
study in the quantum neural network model. First, the feature map applies Hadamard gates on
each of the S := si, qubits, followed by a layer of RZ-gates, whereby the angle of the Pauli rotation
on qubit i depends on the i* feature x; of the data vector Z, normalised between [-1,1].21 Then,
RZZ-gates are implemented on qubits i, i+ 9 for 2 € [1,...,S — 1] and 9g € [i+ 1,...,S] using a
decomposition into two CNOT-gates and one RZ-gate with a rotation angle (a — a;) (m — @i4;).
We consider only up to second order data encoding and the parameterised RZ and RZZ-gates are
repeated once. In other words, the feature map depth is equal to 2 and the operations after the
Hadamard gates in the circuit depicted in Figure 4 are applied again. The classically simulable
feature map employed in the easy quantum model, is simply the first sets of Hadamard and RZ-
gates, as done in Figure 4 and is not repeated.

 

 

 

 

 

 

 

 

 

 

|o) RZ(a1)
(0) b[R@2) 4
I) RZ(z2) BJ R2(2123) -B—-B RZ(¢202) -—

 

‘ar
\0} RZ(es) —® Db

 

 

 

Figure 4: Feature map from [39], used in the quantum neural network. First, Hadamard gates
are applied to each qubit. Then, normalised feature values of the data are encoded using RZ-gates.
This is followed by CNOT-gates and higher order data encoding between every pair of qubits, and
every pair of features in the data. The feature map is repeated to create a depth of 2. The easy
quantum model, introduced in Section 2, applies only the first sets of Hadamard and RZ-gates.

A.2 The variational form

Figure 5 depicts the variational form, deployed in both the easy quantum model and the quantum
neural network. The circuit consists of S qubits, to which parameterised RY-gates are applied.

 

21'This is to be consistent with the chosen parameter space for the classical models.

11
Thereafter, CNOT-gates are applied between every pair of qubits in the circuit. Lastly, another set
of parameterised RY-gates are applied to each qubit. This circuit has, by definition, a depth of 1
and 2S parameters. If the depth is increased, the entangling layers and second set of parameterised
RY-gates are repeated. The number of trainable parameters d can be calculated as d = (D+1)S,
where S is equal to the input size of the data sj, due to the choice of both feature maps used
in this study and D is the depth of the circuit (i.e. how many times the entanglement and RY
operations are repeated).

 

 

 

 

 

 

 

 

 

 

 

eee
\0.) —frven)}/-—e [RY (@s+2) R——
lo) —frv(ea) oo {RY (@s42) ——
(0) —]R¥ (6s) —4

Figure 5: Variational circuit used in both quantum models is plotted in this figure. The circuit
contains parameterised RY-gates, followed by CNOT-gates and another set of parameterised RY-
gates.

B_ Properties of the effective dimension

B.1 Proof of Theorem 3.2

Given a positive definite matrix A > 0, and a function g : Rt — R?*, we define g{A) as the
matrix obtained by taking the image of the eigenvalues of A under the map g. In other words,
A= U'tdiag(j4,...,4a)U implies g( A) = U'diag(g(y1),...,9(4a))U. To prove the assertion of the
theorem, we start with a lemma that relates the effective dimension to the covering number.

Lemma B.1. Let @ = [—1,1]", and let N(e) denote the number of boxes of side length ¢ required
to cover the parameter set O, the length being measured with respect to the metric Fi;(@). Under
the assumption of Theorem 3.2, there exists a dimensional constant cq < co such that for y € (0, 1]

and for alln EN, we have
N 27 logn < eq yn dyn /2
yn 27 logn

Proof. The result follows from the arguments presented in [20]. More precisely, thanks to the
bound ||V¢ log F'(@)|| < A, which holds by assumption, it follows that

 

|) —F(0)|| <caAFO)|| and ||P) — F(0)|| S cal F(8)|| VAC. 6)

In the following, we set ¢ := \/2mrlogn/{yn). Note that, if B-(@,) is a box centered at #, € ©

and of length ¢ (the length being measured with respect to the metric Fy), then this box contains
(1+ caA)~/?B.(6x), where

B.(O,) = {9 € © : LE(O)- (6 —O,),8 — 9) < e7}.

Up to a rotation, we can diagonalise the Fisher information matrix as F(0) = diag(s?,..., 53).
Then, we see that n the number of boxes of the form (1 + cah)~/?B.(65) needed to cover © is

12
given by”?

 

 

= éq(1 + egA)"/?, | det (ite a” F))

 

<éa(1 + ca (ia. + Tropa + cah)P(@))

<6é d : yr
< ég(1 + cagA)", j det (ia + tt ie)) ;

where the second inequality follows from (6) and the fact that the determinant is operator monotone
on the set of positive definite matrices, ie., 0 <A < B implies det(A) < det(B) (55, Exercise 12
in Section 82].

Since the number of boxes of size < (with respect to the metric Fy) needed to cover O is
bounded by the number of boxes of the form (1+ caA)~!/?B.(@,), averaging the bound above with
respect to 6 € © we proved that

 

yn
27 logn

 

N(e) < éa(1 + cgA)t | det. (ia +
Vo eo

which implies the inequality in the statement of Lemma B.1 by recalling the definition of the

effective dimension and ¢ := 4/27 logn/(yn). oO

Lemma B.2. Let ¢ € (0,1). Under the assumption of Theorem 3.2, we have

P( supine) aioe) <2 ((z5)"") om (See).

where N(e) denotes the number of balls of side length c, with respect to F, required to cover the
parameter set ©.

Proof. The proof is a slight generalisation of a result found in [56, Chapter 3]. Let S(@) :=
R(@) — R,(). Then

'S(81) — S(62)| < |R(A1) — R(G2)| + |Fen(A1) — Rn(62)| < 2M||01 — G25, (7)

where the final step uses the fact that R(-) as well as R,(-) are a-Holder continuous with constant
M for M = MPMp. To see this recall that by definition of the risk, we find for the observed input
and output distributions r € P(A’) and ¢ € P()), respectively,

Ex IZ (p(yla;1)r(2), aty)) | — Eng L(wlyler 62)r(z), aty))| |
< Eng ||D(plyles1)r(x), a(y)) — L(e(yles62)r(2),a(y)) ||

< MoE, | ||p(y|@; 61)r(@) — p(y|a; 42)r(2) IF |

< Ma \|p(y\a; 61) — p(yl2; 2) 15 Er [lr(@) IF |

= Mz ||p(yla; 1) — plyl#; 92)II5

< M2M¢ ||. — ell, »

\R(1) — R(O2)| =

 

where the third step uses the continuity assumption of the loss function and the fourth step follows
from Hélder’s inequality. The final step uses the Lipschitz continuity assumption of the model.
Equivalently we see that

|Fin(@1) — Rn(O2)| < MoM? ||#1 — O2ll5, -

 

22Here 4 depends on the orientation of [—-1, 1]? with respect to the boxes Be (6,}. In particular ég < 2Vd (the
length of the diagonal of [-1,1]*}, and if the boxes B-(6,) are aligned along the canonical axes, then é4 = 2.

13
Assume that © can be covered by & subsets By,..., By, ie. 0 = By U...U By. Then, for any

e>0dO,
k k
P S(@¢)| >e) =P S(O)| > Ee S(@)| >e), 8
(2piswize) =? (Uspisinize) sd (qpiswize). 0

where the inequality is due to the union bound. Finally, let k = N(()*) and let B,,..., By
be balls of radius (<q)Ve centered at 61,...,@, covering ©. Then the following inequality holds
for alli=1,...,k,

P (sup |S(®)| > :) <P (\s(@,)) > §). (9)

To prove (9), observe that by using (7) we have for any 6 € B,,

a J é

|S(8) — $(:)| < 2M||@ — Bll S 5.

The last inequality implies that, if |S(@)| > ¢, it must be that |$(@;)| > §. This in turns implies (9).
To conclude, we apply Hoeffding’s inequality, which yields

2

P (|S(6)) = $) =P (IR() — Ra(6)| > $) < 2exn (SE). (10)

 

Combined with (8), we obtain

P (sp \s()| = :) <

9cO

P (sup \S(6)| = :)

6EB;

Me iM

lA

P (\$()| = 5)

é \lVe —ner
(az) ee (apr)
where the second step uses (9). The final step follows from (10) and by recalling that k =
Nain"). o

Having Lemma B.1 and Lemma B.2 at hand we are ready to prove the assertion of Theorem 3.2.

Lemma B.2 implies for ¢ = 444 ,/27 logn/(yn)
P( sup |R(@) — Rn(6)| > 4M /20 foan/ 7) |
dco
Qn logn\ 3 16M?r logn
< ——_ es
sew ((SF)") (ae

l/a 4 2
can (78 ) ) ex (-= =)

1

3.
ll

lA
=

ynile By
a nife
l/a Fz 16M2 l
yn tlogn
<4 ——_ —— 11
<tes( =) exp (EY) a)
where the penultimate step uses
(Pen) 5 (2am)
yn = ynile ,
for all A € (0,1] and a@ € (0,1). The final step in (11) uses Lemma B.1. oO

14
Remark B.3 (Improved scaling for relative entropy loss function). The relative entropy is com-
monly used as a loss function. Note that the relative entropy is log-Lipschitz in the first argument
which is better than Hélder continuous.*? As a result we can improve the bound from Lemma B.2

to
P( sup|R@) — R,,(8)| > :) < wv (qi) exp (-=)

by following the proof given above and utilising the log-Lipschitz property of the relative entropy in
its first argument and the fact that the inverse of t|log(t)| behaves like s/|log(s)| near the origin.*+

Remark B.4 (Boundedness assumption of loss function). By utilizing a stronger concentration
bound than Hoeffding’s inequality in (10), one may be able to relax the assumption that the loss
function in Theorem 3.2 has to be bounded.

B.2 Generalisation ability of the effective dimension

In order to assess the effective dimension’s ability to capture generalisation behaviour, we conduct
a numerical experiment similar to work in [48]. Using a feedforward neural network with a single
hidden layer, an input size of si, = 6, output size soy; = 2 and number of trainable weights d = 880,
we train the network on confusion sets constructed from scikit-learn’s make blobs dataset [57].
More concretely, we use 1000 data points and train the network to zero training loss. This is
repeated several times, each time with the data labels becoming increasingly randomised, thereby
creating multiple confusion sets. The network’s size is chosen such that it is able to achieve zero
training error for all confusion sets considered.

We then calculate the effective dimension of the network, using the parameter set produced after
training on each confusion set. If a proposed capacity measure accurately captures generalisation
ability, we would expect to see an increasing capacity as the percentage of randomised labels in
the confusion set increases, until roughly 50% of the labels are randomised. A network requires
more expressive power to fit random labels (i.e. to fit noise), and this is exactly captured by the
effective dimension and plotted in Figure 6.

B.3 Effective dimension converges to maximal rank of Fisher informa-
tion matrix

The effective dimension converges to the maximal rank of the Fisher information matrix denoted
by F := maxgeo re in the limit n — oo. Since the Fisher information matrix is positive semidefinite,
it can be unitarily diagonalised. By definition of the effective dimension, we see that, without loss
of generality, F'(@) can be diagonal, ie. F(@) = diag(A1(6),...,Ar,(@),0 ...,0). Furthermore we
define the normalisation constant

Vo

B= dF Ob’

such that F(0) = BF(@). Let ky = Irloan and consider n to be sufficiently large such that Ky > 1.
By definition of the effective dimension we find

dan = 2log (Zs I 4/det(idg + nF) / log (Kn)

= 2log (= I V+ en 1(8)) «(1 nrg (@)a8) /log (Kn)

 

< 2log (=, kr? (1 + BA1(@)} ... (1 + BArg (@)28) / log (Kn)

 

*3Recall that the function f(t) = ¢log(t) is log-Lipschitz with constant 1, i-e., | f(t) — f(s)| < |é—s|log(|¢ — s|) for
Jé-s| < 1/e.
24More precisely we can choose k = NE) in the proof above.

15
 

0.22 7

0.20 4

0.18 4

0.16 4

0.14 4

0.12 4

normalised effective dimension

0.10 4

0.08 4

 

 

 

0.0 0.1 0.2 0.3 0.4 0.5
% of randomised labels

Figure 6: Generalisation behaviour of the effective dimension. We plot the normalised effective
dimension for a network trained on confusion sets with increasing randomisation, averaged over
10 different training runs, with one standard deviation above and below the mean. The effective
dimension correctly increases as the data becomes “more random” and is thus, able to accurately
capture a model’s generalisation behaviour.

il?
< 2log (a I \/ (1+ 618)... = parla) /log (Kn) ,

where the final step uses that the Fisher information matrix is positive definite. Taking the limit
n — co gives

im din SP + lim 2log (Ge [ve + BAr(8))... (1+ Brr(0))48) /log (Kn) =F.

To see the other direction, let A:= {@ € @: rg =7} and denote its volume by |A|. By definition
of the effective dimension we obtain

lim dyn 2 2log (= I, 4/ det(idg + nF (@))a /log (tn)
= lim 2log(|A|/Ve)/log (rn) + lim 2 log (= I, 4/ det(idg + nF (0) /log (tn)
> lim 2log (= |, Veertnt@)a8) / log (Kn)
=F + lim 2log (= [, vaee*@pa0) / log (Kn)

=F.

This proves the other direction and concludes the proof. O

B.4 A geometric depiction of the effective dimension

The effective dimension defined in (2) does not necessarily increase monotonically with the number
of data, n. Recall that the effective dimension attempts to capture the size of a model, whilst n
determines the resolution at which the model can be observed. Figure 7 contains an intuitive
example of a case where the effective dimension is not monotone in n. We can interpret a model

16
as a geometric object. When n is small, the resolution at which we are able to see this object
is very low. In this unclear, low resolution setting, the model can appear to be a 2-dimensional
disk as depicted in Figure 7. Increasing n, increases the resolution and the model can then look
1-dimensional, as seen by the spiralling line in the medium resolution regime. Going to very
high resolution, and thus, very high n, reveals that the model is a 2-dimensional structure. In
this example, the effective dimension will be high for small n, where the model is considered 2-
dimensional, lower for slightly higher n where the model seems 1-dimensional, and high again as
the number of data becomes sufficient to accurately quantify the true model size. Similar examples
can be constructed in higher dimensions by taking the same object and allowing it to spiral inside
the unit ball of the ambient space R¢. Then, the effective dimension will be d for small n, it will
go down to a value close to 1, and finally converge to 2 as n > oo. In all experiments conducted
in this study, we examine the effective dimension over a wide range of n, to ensure it is sufficient
in accurately estimating the size of a model.

Low resolution Medium resolution High resolution

 

Figure 7: Geometric picture of a model at different resolution scales. In the low resolution
scale, the model can appear as a 2-dimensional disk and the effective dimension attempts to quantify
the size of this disk. As we enhance the resolution by increasing the number of data used in the
effective dimension, the medium scale reveals a 1-dimensional line, spiralling. Adding sufficient
data and moving to high resolution allows the effective dimension to accurately capture the model’s
true size, which in this case is actually a 2-dimensional object. Thus, the effective dimension does
not necessarily increase monotonically with the number of data used.

B.5 Removing the rank constraint via discretisation

The aim of this section is to find a suitable generalisation of the results in Section B.1 when the
Fisher information matrix does not satisfy the bound ||Vg log F’|| < A. Indeed, this is a rather
strong bound as it forces F to have constant rank, so it is desirable to find a variant of Lemmas B.1
and B.2 that do not require such an assumption.

Our approach to this general problem is based on the idea that, in practical applications, the
Fisher matrix is evaluated at finitely many points, so it makes sense to approximate a statisti-
cal model with a discretised one where the corresponding Fisher information matrix is piecewise
constant.

Let © = [-1,1]? and consider a statistical model Mo := {p(-,;9) : @ € O} with a Fisher
information matrix denoted by F(6) for 6 € @. Given an integer « > 1, we consider a discretised
version of the statistical model. More precisely, we split © into «@ disjoint cubes {Gi}, of size
2/«. Then, given one of these small cubes G;, we consider its center x; and we split G; into Q¢
disjoint simplices, where each simplex is generated by x; and one of the faces of OG;. We denote the
set of all these simplices by {O¢}7~,, where m = 24«%. Note that {Oe}7~, is a regular triangulation
of 0.

Now, let Me = {p'")(.,-0): 8 € QO} be a discretised version of Mo such that p\) is affine on
each simplex @,.2° Note that, with this definition, the Fisher information matrix of the discretised
model F'“) (9) is constant inside each simplex @». We note that, by construction, 8 p\“)(-, 5 6) is

 

25For this, it suffices to define p(., -;0) = p(-,-;@) whenever @ coincides with one of the vertices of O, for some
£, and then one extends p‘*) inside each simplex ©, as an affine function.

17
still M,-Lipschitz continuous.° The risk function with respect to the discretised model is denoted

by R™,

Theorem B.5 (Generalisation bound for effective dimension without rank constraint). Let @ =
[-1,1]¢ and consider a statistical model Mo := {pl-,-;9) : 6 € O} satisfying (4), For x € N,
let Me) : = {p\)(.,-;0):9€ O} be the discretised form as described above. Let ds), denote the
effective dimension of Ms) as defined in (2). Furthermore, let L : P(V) x P(Y) > [-B/2, B/2|
for B > 0 be a loss function that is a-Hélder continuous with constant M2 in the first argument
w.r.t. the total variation distance for some a € {0,1]. Then, there exists a dimensional constant
cq such that for y € (0,1) and for alln CN, we have

go)

1/ aa 2

ar logn ynel 2 16M*nx logn

P (9) — R®)(6)| > 4M, f ——= | <eg ( — —_— 12
(spin (8) — Ry” (8)| = sn) <8 \ Smognive exp By , (12)

where M = Mf'Mo.

To prove the statement of the theorem we need a preparatory lemma that is the discretised
version of Lemma B.1.

Lemma B.6. Let 9 = {—1,1]¢, and let N“*)(e) denote the number of boxes of side len, i € required
to cover the parameter set Q, the length being measured with respect to the metric BAS (@). Under
the assumption of Theorem B.5, there exists a dimensional constant cq < co such that vat y € (0, 1]
and for alln EN, we have

an /2
Nw) an logn < eq yn
yn 27 logn

Proof. Recall that we work in the discretised model ME ), so our metric FAS) (@) is constant on
each element ©, of the partition. So, we fix 2, and we count first the number of boxes of side length
€ required to cover Og.

Up to a rotation, we can diagonalise the Fisher information matrix fF“) le, as diag(s
Note that @, has Euclidean diameter bounded by 2«~! and volume «~¢. Also, if B.(8e
centered at 6 € Og and of length ¢, then

 

Ty 8a):
) is a ball

d

B-(82) 1 Oe := {9 € Oe : 5 s?[(0 — 6p) - ei]? < *}.

wl

Then, the number of balls of size « needed to cover QO, is bounded by

 

- cant | ,/det (ida + e-2F*(8) a8,
Oe

where Gg is a positive dimensional constant, and the last equality follows from the fact that the
volume of ©» is equal to «@ and that F(*) is constant on Ov.
Summing this bound over = 1,...,m, we conclude that (note that Vo = 24)

1 -
Ce \< d 26) (6 — 6,4¢— : —2 F(x)
N ea2 » ‘Lh, det (ida + e- B (8) )ae ea" I det (ida +e Ft)(0) ) ag

 

26Indeed, recall that we defined p‘“) = p on the vertices of the simplices and then we extended p‘*) as an affine
function inside each simplex. With this construction, the Lipschitz constant of p‘*) is bounded by the Lipschitz
constant of p (since the affine extension does not increase the Lipschitz constant}.

18
Applying this bound with e = \/2mlogn/{yn) and recalling the definition of the effective dimen-
sion, the result follows. Oo

Proof of Theorem B.5. We start be noting that Lemma B.2 remains valid for the discretised setting
and under the assumption of Theorem B.5,7" ie.,

oe 2
P (sup R (6) — RIM(@)| > 2) < 2 ((22)""") exp (2), (13)

9cO

where N (2) denotes the number of balls of side length ¢, with respect to Fle), required to cover
the parameter set ©. This can be seen by going through the proof of Lemma B.2. Hence, by

Lemma B.6, we find for ¢ = 4M 4/2n logn/(yn)
P( sup |R%) (6) — R&*(8)| > 4M \/2r log nn) |

GeO
l/a Fz 16M2 l
yn nt logn
<4 ——_ —_———_ ].. 14
~ «(stear) exp ( Bey ) a4

O

Remark B.7 (How to choose the discretisation parameter «). In this remark we discuss conditions
such that the generalisation bound of Theorem B.5 for the discretised model Me) is a good
approximation to a generalisation bound of the original model Mo. Assume that the model Mo
satisfies an additional regularity assumption of the form ||Vo£()|| < A for some A > 0 and for
all 6 € ©, then choosing the discretisation parameter « >> A ensures that that Mo = Me and
F(6) = F(6). Furthermore, “7 >> « is required to ensure that the balls used to cover each
simplex of the triangulation are smaller than the size of each simplex.

C Numerical experiments

C.1 Sensitivity analysis for the effective dimension

We use Monte Carlo sampling to estimate the effective dimension. The capacity results will, thus,
be sensitive to the number of data samples used in estimating the Fisher information matrix for a
given @, and to the number of @ samples then used to calculate the effective dimension. We plot
the normalised effective dimension with n fixed, in Figure 8 over an increasing number of data
and parameter samples using the classical feedforward model. For networks with less trainable
parameters, d, the results stabilise with as little as 40 data and parameter samples. When higher
dimensions are considered, the standard deviation around the results increases, but 100 data and
parameter samples are still reasonable given that we consider a maximum of d = 100. For higher
d, it is likely that more samples will be needed.

C.2 The Fisher information spectra for varying model size

Figure 9 plots the average distribution of the Fisher information eigenvalues for all model types,
over increasing input size, sj,, and hence, increasing number of parameters, d. These average
distributions are generated using 100 Fisher information matrices with parameters, #, drawn uni-
formly at random on 9 = [—1, 1)“. Row A contains the histograms for models with sin = 6, row
B for si, = 8 and row C for sj, = 10. In all scenarios, the classical model has a majority of its
eigenvalues near or equal to zero, with a few very large eigenvalues. The easy quantum model
has a somewhat uniform spectrum for a smaller input size, but this deteriorates as the input size
{also equal to the number of qubits in this particular model) increases. The quantum neural net-
work, however, maintains a more uniform spectrum over increasing sj, and d, showing promise in
avoiding unfavourable qualities, such as barren plateaus.

 

27Lemma B.2 does not require the full rank assumption of the Fisher information matrix.

19
n=1c4 n=1c7 n=1c13

 

 

 

      

 

 

 

 

 

 

 

1.0 1.0 1.0
0.8 0.8 0.84
——
0.6 ES 0.6 SE 6)
FI A
3
3
4 O04 OA 044
a
x
a —sin 4, d 8
5 0.2 0.2 0.2 4 Sq 6, d =12
S s,, —8, d —16
5 —s,, —10, d —20
0.0 ' ' ' ' 0.0 ' 1 r 1 + 0.0 r 1 1 r 1
D 20 40 60 80 100 20 10 60 80 100 20 40 60 80 100
z
Z 10 1.0 1.0
3
z
a
0.8 0.8 0.8 |
3
5
0.6 0.6 0.64
el —
0.4 04 0.4
Sn 4, d —40
0.2 0.2 0.24 Sin =6, d =60
Sin —8, d —80
—s,, =10, d =100
0.0 r r 1 r 0.0 r 1 r r 0.0 r r r r 1
20 40 60 80-100 20 40 60 so 100 20 40 60 so 100

number of data and parameter samples used

Figure 8: Sensitivity analysis of the normalised effective dimension to different numbers of
data and parameter samples, used in calculating the empirical Fisher information matrix, and
subsequently, the effective dimension.

C.3 Training the models using a simulator

To test the trainability of all three model types, we conduct a simple experiment using the Iris
dataset. In each model, we use an input size of si, = 4, output size soy; = 2 and d = 8 trainable
parameters. We train the models for 100 training iterations, using 100 data points from the first
two classes of the dataset. Standard hyperparameter choices are made, using an initial learning
rate = 0.1 and the ADAM optimiser. Each model is trained 100 times, with initial parameters @
sampled uniformly on 6 = [—1, 1]¢ each trial.?® The average training loss and average Fisher-Rao
norm after 100 training iterations, is captured in Table 1. The quantum neural network notably
has the highest Fisher-Rao norm and lowest training loss on average.

 

Model Training loss | Fisher-Rao norm
Classical neural network 37.90% 46.45
Easy quantum model 43.05% 104.89
Quantum neural network 23.14% 117.84

Table 1: Average training loss and average Fisher-Rao norm for all three models, using 100
different trials with 100 training iterations.

 

28We choose 9 = [-1, 1]? as the sample space for the initial parameters, as well as for the parameter sample space
in the effective dimension. Another convention is to use [—27, Qn]? as the parameter space for initialisation of the
quantum model, however, we stick with [-1, 1]? to be consistent and align with classical neural network literature.
We note that for the effective dimension, using either parameter space does affect the observed results.

20
classical neural network

easy quantum model

quantum neural network

 

 

 

 

 

 

 

 

 

 

 

1.0 1.0 1.0
Al A2 A3
0.87 0.8 4 0.8 4
0.67 0.6 4 0.6 4
0.47 0.4 7 0.4 7
0.27 0.2 7 0.2 7
0.07 0.07 0.0 T
0 10 20 30 40 0 1 2 3 4 5 1 2 3
1.0 1.0 1.0
Bl B2 B3
2 084 0.84 0.8 4
<
3
9
. 0.64 0.6 4 0.6 4
~ . . .
o
2
a 0.44 0.4 4 0.4 4
a
°
s
0.27 0.2 4 0.2 4
0.07 0.07 0.0
0 5 10 15 0 2 4 6 8 10 1 2 3
1.0 1.0 1.0
C1 C2 C3
0.84 0.8 4 0.8 4
0.649 0.64 0.6 4
0.44 044 0.4 4
0.24 0.24 0.2 7
0.07 0.0 7 0.0
0 20 40 60 0 2 4 6 8 10 1 2 3 4

 

Figure 9: Average Fisher information spectrum plotted as a histogram for all three model
types, over increasing input size, sin. Row A contains models with si, = 6 and d = 60, row B has
Sin = 8 and d = 80 and row C has si, = 10 and d = 100. In all cases, sou; = 2.

C.4 Training the quantum neural network on real hardware

The hardware experiment is conducted on the ibmqmontreal 27-qubit device. We use 4 qubits
with linear connectivity to train the quantum neural network on the first two classes of the Iris
dataset. We deploy the same training specifications as in Appendix ©.3 and randomly initialise
the parameters. Once the training loss stabilises, i.e. the change in the loss from one iteration to
the next is small, we stop the hardware training. This occurs after roughly 33 training steps. The
results are contained in Figure 3b and the real hardware shows remarkable performance relative
to all other models. Due to limited hardware availability, this experiment is only run once and an
analysis of the hardware noise and the spread of the training loss for differently sampled initial
parameters would make these results more robust.

We plot the circuit that is implemented on the quantum device in Figure 10. As in the quantum
neural network discussed in Appendix A, the circuit contains parameterised RZ and RZZ rotations
that depend on the data, as well as parameterised RY-gates with 8 trainable parameters. Note
the different entanglement structure presented here as opposed to the circuits in Figures 4 and 5.
This is to reduce the number of CNOT-gates required, in order to incorporate current hardware

21
constraints. The full circuit repeats the feature map encoding once before the variational form is
applied.

 

RY(@5

 

 

 

 

 

[0 RZ(e2) RZ(2i22) Hq .—| RY (62) H4 RY (66

 

 

 

 

 

 

 

 

 

 

0) RZ (zs) © RZ(e223) +6 RY @s) fry (6r)
(0) RZ(ea) rz (232) H— [Rv (6) }--_$ rv (os)

Figure 10: Circuit implemented on quantum hardware. First, Hadamard gates are applied.
Then the data is encoded using RZ-gates applied to each qubit whereby the Z-rotations depend
on the feature values of the data. Thereafter, CNOT entangling layers with RZ-gates encoding
products of feature values are applied. The data encoding gates, along with the CNOT-gates are
repeated to create a depth 2 feature map. Lastly, parameterised RY-gates are applied to each
qubit followed by linear entanglement and a final layer of parameterised RY-gates. The circuit has
a total of 8 trainable parameters.

References

[1] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
http://www.deeplearningbook. org.

[2] P. Baldi and R. Vershynin. The capacity of feedforward neural networks. Neural networks,
116:288-311, 2019. DOI: 10.1016/j .neunet .2019.04.009.

[3] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data, 2017.
arXiv:1703.11008.

[4] M.  Schuld. Supervised learning with quantum computers. Springer, 2018.
DOI: 10.1007/978-3-319-96424-9.

[5] C. Zoufal, A. Lucchi, and 8. Woerner. Quantum generative adversarial networks for
learning and loading random distributions. npj Quantum Information, 5{1):1-9, 2019.
DOI: 10.1038/s41534-019-0223-2.

[6] J. Romero, J. P. Olson, and A. Aspuru-Guzik. Quantum autoencoders for efficient
compression of quantum data. Quantum Science and Technology, 2(4):045001, 2017.
DOI: 10.1088/2058-9565/aa8072.

[7] V. Dunjko and H. J. Briegel. Machine learning & artificial intelligence in the quantum
domain: a review of recent progress. Reports on Progress in Physics, 81(7):074001, 2018.
DOI: 10.1088/1361-6633/aab406.

[8] ©. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto, S. Severini, and
L. Wossnig. Quantum machine learning: a classical perspective. Proceedings of the Royal
Society A: Mathematical, Physical and Engineering Sciences, 474(2209):20170551, 2018.
DOI: 10.1098/rspa.2017.0551.

[9] N. Killoran, T. R. Bromley, J. M. Arrazola, M. Schuld, N. Quesada, and 8. Lloyd.
Continuous-variable quantum neural networks. Phys. Rev. Research, 1:033063, 2019.
DOI: 10.1103/PhysRevResearch. 1.033063.

22
[10] M. Schuld, I. Sinayskiy, and F. Petruccione. The quest for a quantum neural network. Quan-
tum Information Processing, 13(11):2567-2586, 2014. DOI: 10.1007/s11128-014-0809-8.

[11] E. Farhi and H. Neven. Classification with quantum neural networks on near term processors.
2018. arXiv:1802.06002.

[12] §. Aaronson. Read the fine print. Nature Physics, 11(4):291-293, 2015.
DOI: 10. 1038/nphys3272.

[13] V. Vapnik. The Nature of Statistical Learning Theory, volume 8, pages 1-15. 2000.
DOI: 10.1007/978-1-4757-3264-1.1.

[14] V.N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability & Its Applications, 16({2):264-280, 1971.
DOI: 10.1137/1116025.

[15] E. D. Sontag. VC dimension of neural networks. NATO ASI Series F Computer and Systems
Sciences, 168:69-96, 1998.

[16] V. Vapnik, E. Levin, and Y. L. Cun. Measuring the VC-dimension of a learning machine.
Neural computation, 6(5):851-876, 1994. DOI: 10.1162/neco.1994.6.5.851.

[17] B. Neyshabur, 8. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in
deep learning. In Advances in neural information processing systems, pages 5947-5956, 2017.
DOI: 10.5555/3295222. 3295344.

[18] S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets
via a compression approach, 2018. arXiv:1802.05296.

[19] L. G. Wright and P. L. McMahon. The capacity of quantum neural networks, 2019.
arXiv:1908.01364.

[20] O. Berezniuk, A. Figalli, R. Ghigliazza, and K. Musaelian. A scale-dependent notion of
effective dimension, 2020. arXiv:2001.10872.

[21] J. J. Rissanen. Fisher information and stochastic complexity. IEEE Transactions on Infor-
mation Theory, 42(1):40-47, 1996. DOI: 10.1109/18.481776.

[22] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley Interscience, 2006.
DOI: 10.1002/047174882x.

[23] J. R. McClean, 8. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven. Barren plateaus
in quantum neural network training landscapes. Nature communications, 9(1):1-6, 2018.
DOI: 10.1038/s41467-018-07090-4.

[24] S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone, L. Cincio, and P. J. Coles. Noise-
induced barren plateaus in variational quantum algorithms. 2020. arXiv:2007.14384.

[25] M. Cerezo, A. Sone, T. Volkoff, L. Cincio, and P. J. Coles. Cost-function-dependent barren
plateaus in shallow quantum neural networks, 2020. arXiv:2001.00550.

[26] G. Verdon, M. Broughton, J. R. McClean, K. J. Sung, R. Babbush, Z. Jiang, H. Neven, and
M. Mohseni. Learning to learn with quantum neural networks via classical neural networks,
2019. arXiv:1907.05415.

[27] T. Volkoff and P. J. Coles. Large gradients via correlation in random parameterized quantum
circuits, 2020. arXiv:2005.12200.

[28] A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib. Layerwise learning
for quantum neural networks, 2020. arXiv:2006.14904.

[29] P. Huembeli and A. Dauphin. Characterizing the loss landscape of variational quantum cir-
cuits, 2020. arXiv:2008.02785.

23
[30]

[31]

[32|

[33]

[35

[36]

(37

[38]

[39]

(40)

(41

[43]
[43]

(44

(46)

(47

C. Bishop. Exact calculation of the Hessian matrix for the multilayer perceptron, 1992.
DOI: 10.1162/neco.1992.4.4.494.

Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Miiller. Efficient BackProp, pages 9-48.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. DOI: 10. 1007/978-3-642-35289-8_3.

M. Cerezo and P. J. Coles. Impact of barren plateaus on the Hessian and higher order
derivatives, 2020. arXiv:2008.07454.

F. Kunstner, P. Hennig, and L. Balles. Limitations of the  empir-
ical Fisher approximation for natural gradient descent. In Advances
in Neural Information Processing Systems 32, pages 4156-4167. 2019.
http://papers .nips.cc/paper/limitations-of-fisher-approximation.

R. Karakida, S. Akaho, and S.-I. Amari. Universal statistics of Fisher informa-
tion in deep neural networks: Mean field approach. volume 89 of Proceedings
of Machine Learning Research, pages 1032-1041. PMLR, 2019. Available online:
http://proceedings .mlr. press/v89/karakida19a. html.

M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe. Circuit-centric quantum classifiers.
Physical Review A, 101(3):032308, 2020. DOI: 10.1103/PhysRevA. 101.032308.

M. Schuld, R. Sweke, and J. J. Meyer. The effect of data encoding on the expressive power of
variational quantum machine learning models, 2020. arXiv:2008.08605.

S. Lloyd, M. Schuld, A. Ijaz, J. Izaac, and N. Killoran. Quantum embeddings for machine
learning, 2020. arXiv:2001.03622.

I. Cong, 8. Choi, and M. D. Lukin. Quantum convolutional neural networks. Nature Physics,
15(12):1273-1278, 2019. DOI: 10. 1038/s41567-019-0648-8.

V. Havliéek, A. D. Cércoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and
J. M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature,
567(7747):209-212, 2019. DOI: 10. 1038/s41586-019-0980-2.

S. Sim, P. D. Johnson, and A. Aspuru-Guzik. Expressibility and entangling capability of
parameterized quantum circuits for hybrid quantum-classical algorithms. Advanced Quantum
Technologies, 2(12):1900070, 2019. DOI: 10.1002/qute. 201900070.

B. R. Frieden. Science from Fisher Information: A Unification. Cambridge University Press,
2004. DOI: 10.1017/CB09780511616907.

P. D. Grinwald. The minimum description length principle. MIT press, 2007.

S.-I. Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998. DOT: 10.1162/089976698300017 746.

T. Liang, T. Poggio, A. Rakhlin, and J. Stokes. Fisher-Rao metric, geometry, and complexity
of neural networks. volume 89 of Proceedings of Machine Learning Research, pages 888-896.
PMLIR, 2019. Available online: http: //proceedings.mlr.press/v89/liang19a.html.

B. Neyshabur, R. R. Salakhutdinov, and N. Srebro. Path-sgd: Path-normalized optimization
in deep neural networks. In Advances in Neural Information Processing Systems, pages 2422—
2430, 2015. DOT: 10.5555/2969442 .2969510.

B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks.
volume 40 of Proceedings of Machine Learning Research, pages 1376-1401, Paris, France, 2015.
PMLR. Available online: http: //proceedings .mlr.press/v40/Neyshabur15.html.

P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems 30, pages 6240-6249.
Curran Associates, Inc., 2017. http://papers.nips.cc/paper/7204-spectrally-normalized.

24
[48]

[49]

[50]

(51

Z. Jia and H. Su. Information-theoretic local minima characterization and regularization,
2019. arXiv:1911.08192.

A. Virmaux and K. Scaman. Lipschitz regularity of deep neural networks: analysis and efficient
estimation. In Advances in Neural Information Processing Systems 31, pages 3835-3844. 2018.
http://papers .nips.cc/paper/lipschitz-regularity-of-deep-neural-networks.

R. Sweke, F. Wilde, J. J. Meyer, M. Schuld, P. K. Fahrmann, B. Meynard-Piganeau, and
J. Eisert. Stochastic gradient descent for hybrid quantum-classical optimization. Quantum,
4:314, 2020. DOI: 10.22331/q-2020-08-31-314.

J. Pennington and P. Worah. The spectrum of the Fisher information ma-
trix of a single-hidden-layer neural network. In Advances in Neural Infor-
mation Processing Systems $31, pages 5410-5419. Curran Associates, Inc., 2018.
http://papers .nips.cc/paper/7786-the-spectrum- of -the-fisher.

Z. Liao, T. Drummond, I. Reid, and G. Carneiro. Approximate fisher information matrix to
characterise the training of deep neural networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, PP:1—1, 2018. DOI: 10.1109/TPAMI.2018. 2876413.

H. Abraham et al. Qiskit: An open-source framework for quantum computing, 2019.
DOI: 10.5281/zenodo. 2562110.

D. Dua and ©. Graff. UCI machine learning repository, 2017. Available online:
http://archive.ics.uci.edu/ml.

P. Halmos. Finite-Dimensional Vector Spaces. Springer-Verlag New York, 1958.
DOI: 10.1007/978-1-4612-6387-6.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press,
2018. Available online: https: //cs.nyu.edu/~mohri/mlbook/.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825-2830, 2011. DOI: 10.5555/1953048. 2078195.

25
